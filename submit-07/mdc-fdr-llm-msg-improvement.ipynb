{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"sourceType":"competition"},{"sourceId":248118764,"sourceType":"kernelVersion"},{"sourceId":166368,"sourceType":"modelInstanceVersion","modelInstanceId":141565,"modelId":164048}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Original Notebook:\n\nhttps://www.kaggle.com/code/yusuketogashi/fork-of-notebooke-llm-msg-improvement","metadata":{}},{"cell_type":"markdown","source":"# Changes\n1. Improving the LLM prompt with few-shot examples.\n\n2. At the end, extract only the lines containing the F1 score.\nThis instructs to filter the final output to show only the relevant lines that include the F1 score information.","metadata":{}},{"cell_type":"code","source":"! uv pip uninstall --system 'tensorflow'\n! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf' 'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'\n! mkdir -p /tmp/src","metadata":{"_uuid":"2ed06cdd-0be1-4bd5-b507-dfc27316b612","_cell_guid":"09357ca4-6420-4ed3-9f70-f5fd62b5fa05","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-23T10:30:05.073675Z","iopub.execute_input":"2025-07-23T10:30:05.073933Z","iopub.status.idle":"2025-07-23T10:30:32.107789Z","shell.execute_reply.started":"2025-07-23T10:30:05.073904Z","shell.execute_reply":"2025-07-23T10:30:32.106849Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 2.36s\u001b[0m\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.18.0\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m157 packages\u001b[0m \u001b[2min 253ms\u001b[0m\u001b[0m=1.26.0                                \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m52 packages\u001b[0m \u001b[2min 23.35s\u001b[0m\u001b[0m                                           \n\u001b[2mUninstalled \u001b[1m14 packages\u001b[0m \u001b[2min 174ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m52 packages\u001b[0m \u001b[2min 214ms\u001b[0m\u001b[0m26.0                          \u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250622\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.9.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.7\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlogits-processor-zoo\u001b[0m\u001b[2m==0.1.12\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.6.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.47b0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions-ai\u001b[0m\u001b[2m==0.4.9\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpymupdf\u001b[0m\u001b[2m==1.26.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==24.0.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.14.7\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.8.5.post1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.29.post2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.18\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile /tmp/src/helpers.py\nimport logging, os, kagglehub, inspect\nfrom pathlib import Path\nimport polars as pl\n\nIS_KAGGLE_ENV = sum(['KAGGLE' in k for k in os.environ]) > 0\nIS_KAGGLE_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\nCOMP_DIR = Path(('/kaggle/input/make-data-count-finding-data-references' if IS_KAGGLE_SUBMISSION else kagglehub.competition_download('make-data-count-finding-data-references')))\nPDF_DIR = COMP_DIR / ('test' if IS_KAGGLE_SUBMISSION else 'train') / 'PDF'\nWORKING_DIR = Path(('/kaggle/working/' if IS_KAGGLE_ENV else '.working/'))\n\nDOI_LINK = 'https://doi.org/'\n\nDEFAULT_LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"DEBUG\").upper() if not IS_KAGGLE_SUBMISSION else \"WARNING\"\nLOG_FILE_PATH = os.getenv(\"LOG_FILE\", \"logs/project.log\")\nLOG_DIR = Path(LOG_FILE_PATH).parent\n\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\nLOG_FORMAT = \"%(levelname)s %(asctime)s  [%(filename)s:%(lineno)d - %(funcName)s()] %(message)s\"\nLOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\n\ndef get_logger(name=None):\n    if name is None:\n        frame = inspect.currentframe()\n        if frame is None or frame.f_back is None:\n            name = \"__main__\"\n        else:\n            name = frame.f_back.f_globals.get(\"__name__\", \"__main__\")\n\n    logger = logging.getLogger(name)\n\n    if not logger.handlers:\n        logger.setLevel(DEFAULT_LOG_LEVEL)\n        formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=LOG_DATEFMT)\n        ch = logging.StreamHandler()\n        ch.setLevel(DEFAULT_LOG_LEVEL)\n        ch.setFormatter(formatter)\n        fh = logging.FileHandler(LOG_FILE_PATH)\n        fh.setLevel(DEFAULT_LOG_LEVEL)\n        fh.setFormatter(formatter)\n        logger.addHandler(ch)\n        logger.addHandler(fh)\n        logger.propagate = False\n    return logger\n\ndef is_doi_link(name: str) -> pl.Expr:\n    return pl.col(name).str.starts_with(DOI_LINK)\n\ndef string_normalization(name: str) -> pl.Expr:\n    return pl.col(name).str.normalize(\"NFKC\").str.replace_all(r\"[^\\p{Ascii}]\", '').str.replace_all(r\"https?://zenodo\\.org/record/(\\d+)\", r\" 10.5281/zenodo.$1 \")\n\ndef get_df(parse_dir: str):\n    records = []\n    txt_files = list(Path(parse_dir).glob('*.txt'))\n    for txt_file in txt_files:\n        id_ = txt_file.stem\n        with open(txt_file, 'r') as f:\n            text = f.read()\n        records.append({'article_id': id_, 'text': text})\n    return pl.DataFrame(records).with_columns(string_normalization('text').alias('text'))\n\ndef assume_type(df: pl.DataFrame) -> pl.DataFrame:\n    return (\n        df.with_columns(pl.when(is_doi_link('dataset_id').or_(pl.col('dataset_id').str.starts_with('SAMN'))).then(pl.lit('Primary')).otherwise(pl.lit('Secondary')).alias('type'))\n    )\n\ndef score(df, gt, on, tag='all'):\n    hits = gt.join(df, on=on)\n    tp = hits.height\n    fp = df.height - tp\n    fn = gt.height - tp\n    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n    return f\"{tag} - f1: {f1:.4f} [{tp}/{fp}/{fn}]\"\n\ndef evaluate(df, on=['article_id', 'dataset_id']):\n    gt = pl.read_csv(COMP_DIR/'train_labels.csv').filter(pl.col('type')!='Missing')\n    return (\n        score(df, gt, on),\n        score(df.filter(is_doi_link('dataset_id')), gt.filter(is_doi_link('dataset_id')), on, 'doi'),\n        score(df.filter(~is_doi_link('dataset_id')), gt.filter(~is_doi_link('dataset_id')), on, 'acc'),\n    )","metadata":{"_uuid":"12432a7e-2525-492a-aa1a-eca4027b47e8","_cell_guid":"dbe46c50-0f60-4056-bf2d-c04b44b13dac","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-23T10:30:32.109581Z","iopub.execute_input":"2025-07-23T10:30:32.109838Z","iopub.status.idle":"2025-07-23T10:30:32.117639Z","shell.execute_reply.started":"2025-07-23T10:30:32.109813Z","shell.execute_reply":"2025-07-23T10:30:32.116858Z"}},"outputs":[{"name":"stdout","text":"Writing /tmp/src/helpers.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile /tmp/src/parse.py\nimport argparse\nfrom pathlib import Path\nimport pymupdf\nfrom helpers import get_logger, PDF_DIR\n\nl = get_logger()\n\ndef pdf_to_txt(output_dir: Path):\n    output_dir.mkdir(parents=True, exist_ok=True)\n    pdf_files = list(PDF_DIR.glob(\"*.pdf\")) + list(PDF_DIR.glob(\"*.PDF\"))\n    existing_txt_files = {f.stem for f in output_dir.glob(\"*.txt\")}\n    for pdf_file in pdf_files:\n        txt_file = output_dir / f\"{pdf_file.stem}.txt\"\n        if pdf_file.stem in existing_txt_files:\n            continue\n        try:\n            text = \"\"\n            with pymupdf.open(pdf_file) as doc:\n                for page in doc:\n                    text += page.get_text()\n            txt_file.write_text(text, encoding='utf-8')\n        except Exception:\n            pass\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('output_dir', type=Path, help='Directory to save text files')\n    args = parser.parse_args()\n    pdf_to_txt(args.output_dir)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"494f9d54-4ffa-49f7-a5a8-9754c16a581e","_cell_guid":"af72dd3c-bee1-44fd-bf1e-831efb12367f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-23T10:30:32.118392Z","iopub.execute_input":"2025-07-23T10:30:32.118606Z","iopub.status.idle":"2025-07-23T10:30:32.166316Z","shell.execute_reply.started":"2025-07-23T10:30:32.118587Z","shell.execute_reply":"2025-07-23T10:30:32.165575Z"}},"outputs":[{"name":"stdout","text":"Writing /tmp/src/parse.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile /tmp/src/check_parse.py\nimport polars as pl\nfrom pathlib import Path\nfrom helpers import *\n\nl=get_logger()\n\ndef gt_dataset_id_normalization(name:str) -> pl.Expr:\n    return (\n        pl.when(is_doi_link(name))\n        .then(pl.col(name).str.split(DOI_LINK).list.last())\n        .otherwise(name)\n        .str.to_lowercase()\n    )\n\ndef main():\n    if IS_KAGGLE_SUBMISSION:\n        l.debug('skipping check_parse for submission')\n        return\n    df = (\n        get_df('/tmp/train_parse')\n        .with_columns(pl.col('text').str.replace_all('\\s+', '').str.to_lowercase().alias('text'))\n    )\n\n    gt = (\n        pl.read_csv(COMP_DIR/'train_labels.csv')\n        .filter(pl.col('article_id').is_in(df['article_id']))\n        .filter(pl.col('type')!='Missing')\n        .with_columns(gt_dataset_id_normalization('dataset_id').alias('norm_id'))\n    )\n\n    l.info(f\"pymupdf misses: {gt.join(df, on='article_id').with_columns(hit=pl.col('text').str.contains(pl.col('norm_id'), literal=True)).filter(~pl.col('hit')).height} dataset_ids\")\n\nif __name__=='__main__': main()","metadata":{"_uuid":"511d19c1-b983-4f18-acb6-624111e17ce5","_cell_guid":"3856377b-8e2d-417d-8899-f758a6416b34","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-23T10:30:32.168164Z","iopub.execute_input":"2025-07-23T10:30:32.168630Z","iopub.status.idle":"2025-07-23T10:30:32.179426Z","shell.execute_reply.started":"2025-07-23T10:30:32.168606Z","shell.execute_reply":"2025-07-23T10:30:32.178737Z"}},"outputs":[{"name":"stdout","text":"Writing /tmp/src/check_parse.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile /tmp/src/getid.py\nimport re\nimport polars as pl\nfrom pathlib import Path\nfrom typing import Optional, Tuple\n\nfrom helpers import *\n\nCOMPILED_PATTERNS = {\n'ref_header_patterns': [re.compile(r'\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE CITED|WORKS CITED|CITED WORKS|ACKNOWLEDGEMENTS)\\b[:\\s]*', re.IGNORECASE)],\n'citation_pattern': re.compile(r'^\\s*(\\[\\d+\\]|\\(\\d+\\)|\\d+\\.|\\d+\\)|\\d+(?=\\s|$))\\s*'),\n'first_citation_patterns': [re.compile(r'^\\s*\\[1\\]\\s*'), re.compile(r'^\\s*\\(1\\)\\s*'), re.compile(r'^\\s*1\\.\\s*'), re.compile(r'^\\s*1\\)\\s*'), re.compile(r'^\\s*1(?=\\s|$)')],\n}\n\nl = get_logger()\n\ndef find_last_reference_header(text: str, header_patterns: list[re.Pattern]) -> Optional[int]:\n    last_match_idx = None\n    for pattern in header_patterns:\n        matches = list(pattern.finditer(text))\n        if matches: last_match_idx = matches[-1].start()\n    return last_match_idx\n\ndef find_last_first_citation(text: str) -> Optional[int]:\n    lines = text.splitlines()\n    last_match_line = None\n    for line_num, line in enumerate(lines):\n        line = line.strip()\n        for pattern in COMPILED_PATTERNS['first_citation_patterns']:\n            if pattern.match(line):\n                next_lines = lines[line_num:line_num+3]\n                if any(COMPILED_PATTERNS['citation_pattern'].match(l.strip()) for l in next_lines[1:]):\n                    last_match_line = line_num\n                break\n    return last_match_line\n\ndef find_reference_start(text: str) -> Optional[int]:\n    lines = text.splitlines()\n    last_first_citation = find_last_first_citation(text)\n    if last_first_citation is not None: return last_first_citation\n    start_search_idx = int(len(lines) * 0.5)\n    for i in range(start_search_idx, len(lines)):\n        line = lines[i].strip()\n        if COMPILED_PATTERNS['citation_pattern'].match(line):\n            next_lines = lines[i:i+3]\n            if sum(1 for l in next_lines if COMPILED_PATTERNS['citation_pattern'].match(l.strip())) >= 2:\n                for j in range(i, max(-1, i-10), -1):\n                    if not COMPILED_PATTERNS['citation_pattern'].match(lines[j].strip()): return j + 1\n                return max(0, i-10)\n    return None\n\ndef split_text_and_references(text: str) -> Tuple[str, str]:\n    header_idx = find_last_reference_header(text, COMPILED_PATTERNS['ref_header_patterns'])\n    if header_idx is not None:\n        header_idx2 = find_last_reference_header(text[:header_idx].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n        if header_idx2 is not None:\n            header_idx3 = find_last_reference_header(text[:header_idx2].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n            if header_idx3 is not None: return text[:header_idx3].strip(), text[header_idx3:].strip()\n            return text[:header_idx2].strip(), text[header_idx2:].strip()\n        return text[:header_idx].strip(), text[header_idx:].strip()\n    ref_start_line = find_reference_start(text)\n    if ref_start_line is not None:\n        lines = text.splitlines()\n        body = '\\n'.join(lines[:ref_start_line])\n        refs = '\\n'.join(lines[ref_start_line:])\n        return body.strip(), refs.strip()\n    return text.strip(), ''\n\ndef get_splits(df):\n    main_texts, ref_texts = [], []\n    for raw_text in df['text']:\n        main, refs = split_text_and_references(raw_text)\n        main_texts.append(main)\n        ref_texts.append(refs)\n    df = df.with_columns(pl.Series('body', main_texts), pl.Series('ref', ref_texts))\n    return df\n\ndef tidy_extraction(df) -> pl.DataFrame:\n    bad_ids = [f'{DOI_LINK}{e}' for e in ['10.5061/dryad', '10.5281/zenodo', '10.6073/pasta']]\n    doi_df = (df.with_columns(pl.col('body').str.extract_all(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+').alias('match')).explode('match').drop_nulls('match').with_columns(pl.col('match').str.replace_all(r'\\s+', '').str.replace(r'[^A-Za-z0-9]+$', '').str.to_lowercase().alias('dataset_id')).group_by('article_id','dataset_id').agg('match').with_columns((DOI_LINK+pl.col('dataset_id')).alias('dataset_id')))\n    acc_df = (df.with_columns(pl.col('text').str.extract_all(r'(?i)\\b(?:CHEMBL\\d+|E-GEOD-\\d+|E-PROT-\\d+|EMPIAR-\\d+|ENSBTAG\\d+|ENSOARG\\d+|EPI_ISL_\\d{5,}|EPI\\d{6,7}|HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|KX\\d{6}|K0\\d{4}|PRJNA\\d+|PXD\\d+|SAMN\\d+|dryad\\s*\\.\\s*[^\\s\"<>]+|pasta\\s*/\\s*[^\\s\"<>])').alias('match')).explode('match').drop_nulls('match').with_columns(pl.col('match').str.replace_all(r'\\s+', '').str.replace(r'[^A-Za-z0-9]+$', '').alias('dataset_id')).group_by('article_id','dataset_id').agg('match').with_columns(pl.when(pl.col('dataset_id').str.starts_with('dryad.')).then(f'{DOI_LINK}10.5061/' + pl.col('dataset_id')).otherwise('dataset_id').alias('dataset_id')).with_columns(pl.when(pl.col('dataset_id').str.starts_with('pasta/')).then(f'{DOI_LINK}10.6073/' + pl.col('dataset_id')).otherwise('dataset_id').alias('dataset_id')))\n    df = pl.concat([doi_df, acc_df])\n    df = (df.unique('dataset_id').filter(~pl.col('article_id').str.replace('_','/').str.contains(pl.col('dataset_id').str.split(DOI_LINK).list.last().str.escape_regex())).filter(~pl.col('dataset_id').str.contains(pl.col('article_id').str.replace('_','/').str.escape_regex())).filter(~pl.col('dataset_id').str.contains('figshare', literal=True)).filter(~pl.col('dataset_id').is_in(bad_ids)).filter(pl.when(is_doi_link('dataset_id').and_(pl.col('dataset_id').str.split('/').list.last().str.len_chars()<5)).then(False).otherwise(True)).with_columns(pl.col('match').list.unique()))\n    return df\n\ndef get_context_window(text: str, substring: str, window: int = 100) -> str:\n    index = text.find(substring)\n    if index == -1: raise ValueError\n    start = max(index - window, 0)\n    end = min(index + len(substring) + window, len(text))\n    return text[start:end]\n\ndef get_window_df(text_df, ids_df):\n    df = ids_df.join(text_df, on='article_id')\n    windows = []\n    for text, match_ids in df.select('text', 'match').rows():\n        windows.append(get_context_window(text, match_ids[0]))\n    return df.with_columns(pl.Series('window', windows)).select('article_id', 'dataset_id', 'window')\n\n\ndef main():\n    text_df = get_df('/tmp/train_parse')\n    df = get_splits(text_df)\n    df = tidy_extraction(df)\n    df = get_window_df(text_df, df)\n    df.write_parquet('/tmp/extracted.parquet')\n    df = assume_type(df)\n    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n    if not IS_KAGGLE_SUBMISSION:\n        results = evaluate(df)\n        for r in results: l.info(r) \n        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n        for r in results: l.info(r) \n\nif __name__=='__main__': main()","metadata":{"_uuid":"c02f5a2e-8450-4282-95ad-6334f872c660","_cell_guid":"e061c0f2-272a-4c6d-98db-bd9bd27209ab","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-23T10:30:32.180298Z","iopub.execute_input":"2025-07-23T10:30:32.180655Z","iopub.status.idle":"2025-07-23T10:30:32.199969Z","shell.execute_reply.started":"2025-07-23T10:30:32.180630Z","shell.execute_reply":"2025-07-23T10:30:32.199214Z"}},"outputs":[{"name":"stdout","text":"Writing /tmp/src/getid.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile /tmp/src/llm_validate.py\nimport polars as pl\nimport os\n\nfrom helpers import *\n\nl = get_logger()\n\nSYS_PROMPT_CLASSIFY_DOI = \"\"\"\nYou are a highly accurate DOI/type classifier. Given a snippet of academic text containing a DOI or accession, choose:\n\n  A) Data — the identifier points directly to research data in a repository  \n  B) Literature — the identifier points to a journal article, book chapter, protocol paper, or other non-data resource  \n\n=== Repository Prefixes ===\nTreat as DATA if the DOI starts with any of:\n  • 10.5061 (Dryad)  \n  • 10.5281 (Zenodo)  \n  • 10.6084 (Figshare)  \n  • 10.24433/ (Mendeley Data)  \n  • 10.17632 (Mendeley Data)  \n  • SRA/E- (e.g. SRP, SRA)  \n  • PRJNA, PRJEB, PRJDB (NCBI BioProject)  \n  • PRIDE:PXD (Proteomics)  \n  • EMBL:E-MTAB, E- (ArrayExpress)  \n\nEverything else is LITERATURE unless you see explicit data-repository context (e.g. “deposited in Dryad under DOI…”).\n\n=== Few-Shot Examples ===\n1) “Raw images are stored on Figshare (DOI 10.6084/m9.figshare.1234567).” → A  \n2) “Sequence reads available under BioProject accession PRJNA765432.” → A  \n3) “As described in Nature Methods (DOI 10.1038/s41592-020-0793-2).” → B  \n4) “See Supplementary Data at Zenodo (10.5281/zenodo.987654).” → A  \n5) “Method details published in J. Proteome Res. DOI: 10.1021/acs.jproteome.0c00845.” → B  \n6) “Data has been uploaded to Dryad (10.5061/dryad.x1y2z3).” → A  \n7) “Referenced paper: DOI 10.1101/2020.01.01.123456 (bioRxiv preprint).” → B  \n8) “Metabolomics data in MetaboLights MTBLS1234.” → A  \n\n=== Instructions ===\n- Use only the identifier itself and its context.  \n- If the DOI prefix is in the list above, always choose A.  \n- If it belongs to a known publisher prefix (e.g. 10.1007, 10.1038, 10.1126, 10.1016…), choose B.  \n- Otherwise, rely on context words (“deposited”, “uploaded”, “archived”) to decide.  \n- Output exactly one letter: A or B, and nothing else.\n\"\"\".strip()\n\ndef build_df():\n    df = pl.read_parquet('/tmp/extracted.parquet')\n    df.filter(~is_doi_link('dataset_id')).select('article_id', 'dataset_id').write_csv('/tmp/accid_sub.csv')\n    return df.filter(is_doi_link('dataset_id'))\n\ndef build_prompt(tokenizer, df):\n    prompts = []\n    for doi, text in df.select('dataset_id', 'window').rows():\n        messages = [{'role':'system','content': SYS_PROMPT_CLASSIFY_DOI}, {'role':'user', 'content': text}]\n        prompts.append(tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False))\n    return df.with_columns(pl.Series('prompt', prompts))\n\nif __name__=='__main__':\n    os.environ[\"VLLM_USE_V1\"] = \"0\"\n    import vllm\n    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n    model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n    llm = vllm.LLM(model_path, quantization='awq', tensor_parallel_size=2, gpu_memory_utilization=0.9, trust_remote_code=True, dtype=\"half\", enforce_eager=True, max_model_len=2048, disable_log_stats=True, disable_custom_all_reduce=True, enable_prefix_caching=True, task='generate')\n    tokenizer = llm.get_tokenizer()\n    df = build_df()\n    df = build_prompt(tokenizer, df)\n    prompts = df['prompt'].to_list()\n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0.1, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n    choices = [max(d, key=d.get) for d in logprobs]\n    types = {'A': True, 'B': False}\n    choices = [types[c] for c in choices]\n    df = df.with_columns(pl.Series('type', choices))\n    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n    df = assume_type(df)\n    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n    if not IS_KAGGLE_SUBMISSION:\n        results = evaluate(df)\n        for r in results: l.info(r) \n        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n        for r in results: l.info(r)","metadata":{"_uuid":"637755bd-7f77-43ba-8ebe-0491f3fd3a5d","_cell_guid":"58bff063-67aa-42bb-9ae2-2fe637f7983c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-23T10:30:32.200939Z","iopub.execute_input":"2025-07-23T10:30:32.201165Z","iopub.status.idle":"2025-07-23T10:30:32.215728Z","shell.execute_reply.started":"2025-07-23T10:30:32.201139Z","shell.execute_reply":"2025-07-23T10:30:32.215034Z"}},"outputs":[{"name":"stdout","text":"Writing /tmp/src/llm_validate.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%cd /tmp\n!LOG_LEVEL=INFO python src/parse.py /tmp/train_parse\n! python src/check_parse.py\n! python src/getid.py\n! python src/llm_validate.py","metadata":{"_uuid":"8fb9d14f-293f-49ce-ac94-e40922695842","_cell_guid":"4d2779de-7c0b-4d87-9603-e961eff5e3d9","trusted":true,"collapsed":false,"scrolled":true,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-23T10:30:32.216376Z","iopub.execute_input":"2025-07-23T10:30:32.216546Z","iopub.status.idle":"2025-07-23T10:37:49.262862Z","shell.execute_reply.started":"2025-07-23T10:30:32.216532Z","shell.execute_reply":"2025-07-23T10:37:49.262036Z"}},"outputs":[{"name":"stdout","text":"/tmp\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nINFO 2025-07-23 10:32:01  [check_parse.py:31 - main()] pymupdf misses: 42 dataset_ids\nINFO 2025-07-23 10:32:07  [getid.py:110 - main()] all - f1: 0.6039 [481/393/238]\nINFO 2025-07-23 10:32:07  [getid.py:110 - main()] doi - f1: 0.4350 [164/265/161]\nINFO 2025-07-23 10:32:07  [getid.py:110 - main()] acc - f1: 0.7557 [317/128/77]\nINFO 2025-07-23 10:32:07  [getid.py:112 - main()] all - f1: 0.5047 [402/472/317]\nINFO 2025-07-23 10:32:07  [getid.py:112 - main()] doi - f1: 0.3395 [128/301/197]\nINFO 2025-07-23 10:32:07  [getid.py:112 - main()] acc - f1: 0.6532 [274/171/120]\nINFO 07-23 10:32:21 [__init__.py:239] Automatically detected platform cuda.\nWARNING 07-23 10:32:37 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 07-23 10:32:37 [config.py:1770] Defaulting to use mp for distributed inference\nWARNING 07-23 10:32:37 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nINFO 07-23 10:32:37 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \nWARNING 07-23 10:32:38 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:32:38 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\nINFO 07-23 10:32:38 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 07-23 10:32:38 [cuda.py:289] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:32:38 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:32:38 [cuda.py:289] Using XFormers backend.\n[W723 10:32:49.898224365 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W723 10:32:50.261404121 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W723 10:32:59.904199931 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W723 10:33:09.914082857 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\nINFO 07-23 10:33:09 [utils.py:1055] Found nccl from library libnccl.so.2\nINFO 07-23 10:33:09 [pynccl.py:69] vLLM is using nccl==2.21.5\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:33:09 [utils.py:1055] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:33:09 [pynccl.py:69] vLLM is using nccl==2.21.5\nINFO 07-23 10:33:10 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_27dc8105'), local_subscribe_addr='ipc:///tmp/3084a8bf-33f3-4843-a07c-1e63fd92372e', remote_subscribe_addr=None, remote_addr_ipv6=False)\nINFO 07-23 10:33:10 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:33:10 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\nINFO 07-23 10:33:10 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:33:10 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:22<01:28, 22.21s/it]\nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:49<01:16, 25.38s/it]\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [01:22<00:57, 28.89s/it]\nLoading safetensors checkpoint shards:  80% Completed | 4/5 [01:58<00:31, 31.58s/it]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [02:33<00:00, 32.93s/it]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [02:33<00:00, 30.78s/it]\n\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:35:44 [loader.py:458] Loading weights took 153.98 seconds\nINFO 07-23 10:35:44 [loader.py:458] Loading weights took 154.24 seconds\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:35:44 [model_runner.py:1140] Model loading took 9.0935 GiB and 154.341963 seconds\nINFO 07-23 10:35:45 [model_runner.py:1140] Model loading took 9.0935 GiB and 154.606848 seconds\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:35:55 [worker.py:287] Memory profiling takes 10.28 seconds\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:35:55 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n\u001b[1;36m(VllmWorkerProcess pid=191)\u001b[0;0m INFO 07-23 10:35:55 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 3.63GiB.\nINFO 07-23 10:35:55 [worker.py:287] Memory profiling takes 10.43 seconds\nINFO 07-23 10:35:55 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\nINFO 07-23 10:35:55 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 2.66GiB.\nINFO 07-23 10:35:56 [executor_base.py:112] # cuda blocks: 1363, # CPU blocks: 2048\nINFO 07-23 10:35:56 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 10.65x\nINFO 07-23 10:36:00 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 14.84 seconds\nProcessed prompts: 100%|█| 429/429 [01:41<00:00,  4.22it/s, est. speed input: 31\nINFO 2025-07-23 10:37:43  [llm_validate.py:81 - <module>()] all - f1: 0.6891 [481/196/238]\nINFO 2025-07-23 10:37:43  [llm_validate.py:81 - <module>()] doi - f1: 0.5889 [164/68/161]\nINFO 2025-07-23 10:37:43  [llm_validate.py:81 - <module>()] acc - f1: 0.7557 [317/128/77]\nINFO 2025-07-23 10:37:43  [llm_validate.py:83 - <module>()] all - f1: 0.5759 [402/275/317]\nINFO 2025-07-23 10:37:43  [llm_validate.py:83 - <module>()] doi - f1: 0.4596 [128/104/197]\nINFO 2025-07-23 10:37:43  [llm_validate.py:83 - <module>()] acc - f1: 0.6532 [274/171/120]\nERROR 07-23 10:37:44 [multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 191 died, exit code: -15\nINFO 07-23 10:37:44 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\n[rank0]:[W723 10:37:45.679598260 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nException ignored in: <Finalize object, dead>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 227, in __call__\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 206, in _finalize_close\nTypeError: 'NoneType' object is not callable\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#! cat /tmp/logs/project.log\n! grep \"f1:\" /tmp/logs/project.log","metadata":{"_uuid":"d7d8ae4d-7d0f-4275-9487-ce20971021bd","_cell_guid":"56f5b7bc-2752-414a-b9e2-cfa313285a78","trusted":true,"collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-23T10:37:49.263929Z","iopub.execute_input":"2025-07-23T10:37:49.264191Z","iopub.status.idle":"2025-07-23T10:37:49.389110Z","shell.execute_reply.started":"2025-07-23T10:37:49.264159Z","shell.execute_reply":"2025-07-23T10:37:49.388233Z"}},"outputs":[{"name":"stdout","text":"INFO 2025-07-23 10:32:07  [getid.py:110 - main()] all - f1: 0.6039 [481/393/238]\nINFO 2025-07-23 10:32:07  [getid.py:110 - main()] doi - f1: 0.4350 [164/265/161]\nINFO 2025-07-23 10:32:07  [getid.py:110 - main()] acc - f1: 0.7557 [317/128/77]\nINFO 2025-07-23 10:32:07  [getid.py:112 - main()] all - f1: 0.5047 [402/472/317]\nINFO 2025-07-23 10:32:07  [getid.py:112 - main()] doi - f1: 0.3395 [128/301/197]\nINFO 2025-07-23 10:32:07  [getid.py:112 - main()] acc - f1: 0.6532 [274/171/120]\nINFO 2025-07-23 10:37:43  [llm_validate.py:81 - <module>()] all - f1: 0.6891 [481/196/238]\nINFO 2025-07-23 10:37:43  [llm_validate.py:81 - <module>()] doi - f1: 0.5889 [164/68/161]\nINFO 2025-07-23 10:37:43  [llm_validate.py:81 - <module>()] acc - f1: 0.7557 [317/128/77]\nINFO 2025-07-23 10:37:43  [llm_validate.py:83 - <module>()] all - f1: 0.5759 [402/275/317]\nINFO 2025-07-23 10:37:43  [llm_validate.py:83 - <module>()] doi - f1: 0.4596 [128/104/197]\nINFO 2025-07-23 10:37:43  [llm_validate.py:83 - <module>()] acc - f1: 0.6532 [274/171/120]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Reslut:\n\nScore: 0.535\n\nRank: 72 (2025-07-23-19:30, JST)\n\nYour Best Entry!\nYour most recent submission scored 0.535, which is an improvement of your previous score of 0.533. Great job!\n\nRank 72 - moving up, just a few more to go. #kaggle - https://kaggle.com/competitions/make-data-count-finding-data-references ","metadata":{}},{"cell_type":"markdown","source":"Make Data Count, Maggie Demkin, and Walter Reade. Make Data Count - Finding Data References. https://kaggle.com/competitions/make-data-count-finding-data-references, 2025. Kaggle.","metadata":{}}]}