{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"sourceType":"competition"},{"sourceId":12323294,"sourceType":"datasetVersion","datasetId":7767828},{"sourceId":12349195,"sourceType":"datasetVersion","datasetId":7785241},{"sourceId":12418719,"sourceType":"datasetVersion","datasetId":7832532},{"sourceId":166368,"sourceType":"modelInstanceVersion","modelInstanceId":141565,"modelId":164048},{"sourceId":391615,"sourceType":"modelInstanceVersion","modelInstanceId":322452,"modelId":322000}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Enhancements based on the baseline: Dataset Mention Extraction üìÑüîç**\n\nThis notebook is an improvement upon the work from: [LB-0.289-improved-basline-prompt-engineering](https://www.kaggle.com/code/kawchar85/lb-0-289-improved-basline-prompt-engineering). Thank u.\n\n**Inspiration for using Regex & Context Chunking:**\n\nInspired by the need to extract dataset accessions and DOIs with high precision, I combined regex with smart context slicing and domain-specific heuristics.\n\n**What I changed:**\n\n1. **Regex-Based Identifier Extraction**\n\n   * Added robust patterns to detect **DOIs**, **GSE/SRA**, **CHEMBL**, **UniProt**, and other dataset-related IDs.\n\n2. **Heuristic Keyword Filtering**\n\n   * Matched surrounding text against known **dataset-related phrases** (e.g., ‚Äúdata available at‚Äù, ‚Äúrepository‚Äù) to filter meaningful mentions.\n\n3. **Smart Contextual Chunking**\n\n   * Implemented a `TextChunker` that aligns context by sentence boundaries, ensuring that extracted snippets are informative and self-contained.\n\n4. **Dataset DOI Classification**\n\n   * Checked matched DOIs against a curated list of known **dataset DOI prefixes** to validate dataset relevance.\n\n5. **Parallel PDF Processing**\n\n   * Boosted performance with **ThreadPoolExecutor**, allowing multiple PDFs to be parsed concurrently.\n\n6. **Model Testing: Non-Reasoning vs Reasoning**\n\n   * This notebook includes evaluation with **Qwen 2.5** for non-reasoning classification and **Qwen 3** for reasoning-intensive classification ‚Äî allowing comparison and ablation between the two modes.\n\n7. **Detailed False Negative (FN) Analysis**\n\n   * Added in-depth analysis to categorize and quantify **False Negatives** (FN), separated into:\n\n     * **Wrongly classified**: Model predicted something, but not exactly correct.\n     * **Completely missed**: No prediction was made for a ground-truth item.\n   * Each group is further broken down into:\n\n     * **DOI**-based errors (e.g., wrong prefix or mismatched)\n     * **Accession ID** errors (e.g., GSE, PRJNA, etc.)\n   * This helps reveal weaknesses such as:\n\n     * Ambiguous contexts\n     * Incomplete extraction logic\n     * Confusions between similar dataset identifiers\n\n**Next Goal:**\n\n1. **Improve Chunk and Reduce Junk Chunk**\n2. **What to Improve Regex**\n3. **Added Prompt Caching**\n4. **Reduce Runtime for run**\n5. **More F1-Score Reduce FN**\n\n**I hope this notebook to goal gold medal notebook**\n","metadata":{}},{"cell_type":"code","source":"#!pip install pymupdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T22:35:34.980364Z","iopub.execute_input":"2025-07-11T22:35:34.980616Z","iopub.status.idle":"2025-07-11T22:37:58.211608Z","shell.execute_reply.started":"2025-07-11T22:35:34.980592Z","shell.execute_reply":"2025-07-11T22:37:58.210666Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78ebe5617510>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymupdf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78ebe549b810>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymupdf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78ebe5750e10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymupdf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78ebe5529d10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymupdf/\u001b[0m\u001b[33m\n\u001b[0m^C\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#pip install pymupdf -i https://pypi.tuna.tsinghua.edu.cn/simple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T22:38:03.906235Z","iopub.execute_input":"2025-07-11T22:38:03.906833Z","iopub.status.idle":"2025-07-11T22:41:24.856908Z","shell.execute_reply.started":"2025-07-11T22:38:03.906796Z","shell.execute_reply":"2025-07-11T22:41:24.855977Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78f2cdbeef90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymupdf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78f2cdd152d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymupdf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78f2cdc03f10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymupdf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78f2cdd2a1d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymupdf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78f2cdc10ed0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pymupdf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pymupdf (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pymupdf\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip list | grep -i pdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T22:41:50.312141Z","iopub.execute_input":"2025-07-11T22:41:50.312901Z","iopub.status.idle":"2025-07-11T22:41:52.726230Z","shell.execute_reply.started":"2025-07-11T22:41:50.312864Z","shell.execute_reply":"2025-07-11T22:41:52.725337Z"}},"outputs":[{"name":"stdout","text":"pdf2image                          1.17.0\npypdf                              5.4.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport re\n#import fitz\nfrom pypdf import PdfReader\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport torch\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport multiprocessing as mp\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nimport vllm\nimport sys\n\nsys.path.append('/kaggle/input/secret-process')\n\nimport my_secret\n\nprint(\"Starting PDF processing using secret code...\")\nchunks, chunks2, ids = my_secret.main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T22:43:32.696892Z","iopub.execute_input":"2025-07-11T22:43:32.697224Z","iopub.status.idle":"2025-07-11T22:43:42.452929Z","shell.execute_reply.started":"2025-07-11T22:43:32.697194Z","shell.execute_reply":"2025-07-11T22:43:42.451936Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2403383916.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlogits_processor_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultipleChoiceLogitsProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'logits_processor_zoo'"],"ename":"ModuleNotFoundError","evalue":"No module named 'logits_processor_zoo'","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"## Load LLM","metadata":{}},{"cell_type":"code","source":"think_mode = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if think_mode:\n    model_path = \"/kaggle/input/qwen-3/transformers/8b-awq/1\"\n    llm = vllm.LLM(\n        model_path,\n        quantization='awq',\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=0.92,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=4096,\n        disable_log_stats=True,\n        enable_prefix_caching=True\n    )\nelse:\n    model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n    llm = vllm.LLM(\n        model_path,\n        quantization='awq',\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=0.92,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=1024+512,\n        disable_log_stats=True,\n        enable_prefix_caching=True\n    )\ntokenizer = llm.get_tokenizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T04:10:31.85982Z","iopub.execute_input":"2025-07-08T04:10:31.860184Z","iopub.status.idle":"2025-07-08T04:16:29.206917Z","shell.execute_reply.started":"2025-07-08T04:10:31.860163Z","shell.execute_reply":"2025-07-08T04:16:29.206287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# System prompts","metadata":{}},{"cell_type":"code","source":"SYS_PROMPT_DOI = \"\"\"\nYou are an expert at identifying RESEARCH DATA citations in academic papers.\nYour task is to determine if a DOI in the provided text specifically refers to a dataset, software, or data repository, NOT another academic paper.\n\n**Crucial Rules:**\n1.  **LOOK FOR DATA CONTEXT:** The DOI must be near keywords like \"data available\", \"deposited in\", \"repository\", \"accession number\", \"software\", \"code\".\n2.  **IGNORE BIBLIOGRAPHY:** If the DOI is clearly part of a numbered or author-year list in a \"References\" or \"Bibliography\" section, you MUST respond with \"Irrelevant\".\n3.  **PRIORITIZE DATA DOIs:** If there are multiple DOIs, return the one most likely to be a dataset.\n\nOnly respond with either a full normalized DOI URL starting with \"https://doi.org/\" or the single word \"Irrelevant\".\nDo NOT include any other text or explanation.\n\"\"\"\n\nif think_mode:\n    \n    SYS_PROMPT_ACCESSION = \"\"\"\n    You are an expert at analyzing research data usage in academic papers.\n    \n    Think step-by-step about the surrounding text, identifying clues such as:\n    - PRIMARY data: ‚Äúwe deposited‚Äù, ‚Äúdata generated in this study‚Äù, ‚Äúour data‚Äù, ‚Äúsubmitted to‚Äù, ‚Äúnewly generated‚Äù\n    - SECONDARY data: ‚Äúdownloaded from‚Äù, ‚Äúobtained from‚Äù, ‚Äúpreviously published‚Äù, ‚Äúpublicly available‚Äù, ‚Äúexisting dataset‚Äù\n    - NONE: mentioned only in references, general methodology descriptions without actual usage, or contexts unrelated to research data\n    \n    Silently reason through the classification.\n    \n    Please show your choice in the answer field with only the choice letter, e.g.,  \n    \"answer\": \"C\"\n    \"\"\"\n    \n    SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n    You are an expert at analyzing research data citations in academic papers.\n    \n    First, reason step-by-step about whether the DOI refers to data that is:\n    A) Primary ‚Äì generated specifically for this study  \n    B) Secondary ‚Äì reused or derived from prior work  \n    C) None ‚Äì merely cited in references, not research data, or otherwise unrelated\n    \n    Perform this reasoning silently.\n    \n    Please show your choice in the answer field with only the choice letter, e.g.,  \n    \"answer\": \"B\"\n    \"\"\"\n\nelse:    \n    SYS_PROMPT_ACCESSION = \"\"\"\n    You are an expert at analyzing research data usage in academic papers.\n    \n    Look for contextual clues:\n    - For PRIMARY data: \"we deposited\", \"data generated in this study\", \"our data\", \"submitted to\", \"newly generated\"\n    - For SECONDARY data: \"downloaded from\", \"obtained from\", \"previously published\", \"publicly available\", \"existing dataset\"\n    - For NONE: mentioned in references, methodology descriptions without actual usage, or unrelated contexts\n    \n    Respond with only one letter: A, B, or C.\n    \"\"\"\n    \n    SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n    You are an expert at analyzing research data citations in academic papers.\n    \n    Classify the data as:\n    A) Primary: if the data was generated specifically for this study\n    B) Secondary: if the data was reused or derived from prior work  \n    C) None: if the DOI is in references, doesn't refer to research data, or is unrelated\n    \n    Respond with only one letter: A, B, or C.\n    \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T04:16:29.209194Z","iopub.execute_input":"2025-07-08T04:16:29.209996Z","iopub.status.idle":"2025-07-08T04:16:29.213738Z","shell.execute_reply.started":"2025-07-08T04:16:29.209972Z","shell.execute_reply":"2025-07-08T04:16:29.213182Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ask LLM to extract DOI links","metadata":{}},{"cell_type":"code","source":"prompts = []\nfor article_id, academic_text in chunks:\n    messages = [\n        {\"role\": \"system\", \"content\": SYS_PROMPT_DOI},\n        {\"role\": \"user\", \"content\": academic_text}\n    ]\n\n    if think_mode:\n\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,enable_thinking=False\n        )\n    else:\n         prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False\n        )\n    \n    prompts.append(prompt)\n\noutputs = llm.generate(\n    prompts,\n    vllm.SamplingParams(\n        seed=0,\n        skip_special_tokens=True,\n        max_tokens=64,\n        temperature=0\n    ),\n    use_tqdm=True\n)\n\nresponses = [output.outputs[0].text.strip() for output in outputs]\n\ndoi_pattern = re.compile(r'(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)', re.I)\n\ndoi_urls = []\nfor response in responses:\n    if response.lower() == \"irrelevant\":\n        doi_urls.append(\"Irrelevant\")\n    else:\n        match = doi_pattern.search(response)\n        if match:\n            doi_urls.append(\"https://doi.org/\" + match.group(1))\n        else:\n            doi_urls.append(\"Irrelevant\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:09:35.834853Z","iopub.execute_input":"2025-07-08T05:09:35.835101Z","iopub.status.idle":"2025-07-08T06:39:06.166719Z","shell.execute_reply.started":"2025-07-08T05:09:35.835085Z","shell.execute_reply":"2025-07-08T06:39:06.16586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef parse_answer_with_regex(response_text: str):\n\n    if not isinstance(response_text, str):\n        return None\n\n    match = re.search(r'answer\\b.*?([ABC])\\b', response_text, re.IGNORECASE | re.DOTALL)\n    if match:\n        return match.group(1)\n\n    all_choices = re.findall(r'[ABC]', response_text)\n    if all_choices:\n        return all_choices[-1]\n        \n    return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompts = []\nvalid_indices = []\n\nif think_mode:\n    for i, (chunk, url) in enumerate(zip(chunks, doi_urls)):\n        if url == \"Irrelevant\":\n            continue\n    \n        article_id, academic_text = chunk\n        messages = [\n            {\"role\": \"system\", \"content\": SYS_PROMPT_CLASSIFY_DOI},\n            {\"role\": \"user\", \"content\": f\"DOI: {url}\\n\\nAcademic text:\\n{academic_text}\"}\n        ]\n    \n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,\n            enable_thinking=True\n        )\n        prompts.append(prompt)\n        valid_indices.append(i)\n    \n    outputs = llm.generate(\n        prompts,\n        vllm.SamplingParams(\n            seed=777,\n            temperature=0.65,\n            top_p=0.95,\n            top_k=20,\n            skip_special_tokens=True,\n            max_tokens=2048+1024,\n            presence_penalty=1.5\n        ),\n        use_tqdm=True\n    )\n\n    choice_to_type_map = {'A': 'Primary', 'B': 'Secondary', 'C': None}\n\n    responses = [output.outputs[0].text.strip() for output in outputs]\n    \n    parsed_doi_choices = [parse_answer_with_regex(resp) for resp in responses]\n    final_doi_answers = [choice_to_type_map.get(choice) for choice in parsed_doi_choices]\n    \n    answers = [None] * len(chunks)\n    for i, answer in zip(valid_indices, final_doi_answers):\n        answers[i] = answer\n    \n    \nelse:\n    for i, (chunk, url) in enumerate(zip(chunks, doi_urls)):\n        if url == \"Irrelevant\":\n            continue\n    \n        article_id, academic_text = chunk\n        messages = [\n            {\"role\": \"system\", \"content\": SYS_PROMPT_CLASSIFY_DOI},\n            {\"role\": \"user\", \"content\": f\"DOI: {url}\\n\\nAcademic text:\\n{academic_text}\"}\n        ]\n    \n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,enable_thinking=False\n        )\n        prompts.append(prompt)\n        valid_indices.append(i)\n    \n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\", \"C\"])\n    \n    outputs = llm.generate(\n        prompts,\n        vllm.SamplingParams(\n            seed=777,\n            temperature=0.05, \n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[mclp],\n            logprobs=len(mclp.choices)\n        ),\n        use_tqdm=True\n    )\n    \n    logprobs = []\n    for lps in [output.outputs[0].logprobs[0].values() for output in outputs]:\n        logprobs.append({lp.decoded_token: lp.logprob for lp in list(lps)})\n    \n    logit_matrix = pd.DataFrame(logprobs)[[\"A\", \"B\", \"C\"]].values\n    choices = [\"Primary\", \"Secondary\", None]\n    answers = [None] * len(chunks)\n    \n    for i, (idx, logit_row) in enumerate(zip(valid_indices, logit_matrix)):\n        max_logit = np.max(logit_row)\n        max_idx = np.argmax(logit_row)\n        \n        if max_logit > -2.0:\n            answers[idx] = choices[max_idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T06:39:06.167649Z","iopub.execute_input":"2025-07-08T06:39:06.167888Z","iopub.status.idle":"2025-07-08T07:18:54.366745Z","shell.execute_reply.started":"2025-07-08T06:39:06.167872Z","shell.execute_reply":"2025-07-08T07:18:54.365903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompts = []\n\nif think_mode:\n    for chunk, acc_id in zip(chunks2, ids):\n        article_id, academic_text = chunk\n        messages = [\n            {\"role\": \"system\", \"content\": SYS_PROMPT_ACCESSION},\n            {\"role\": \"user\", \"content\": f\"Accession ID: {acc_id}\\n\\nAcademic text:\\n{academic_text}\"}\n        ]\n    \n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,\n            enable_thinking=True\n        )\n        prompts.append(prompt)\n    \n    outputs = llm.generate(\n        prompts,\n        vllm.SamplingParams(\n            seed=777,\n            temperature=0.65,\n            top_p=0.95,\n            top_k=20,\n            skip_special_tokens=True,\n            max_tokens=2048+1024,\n            presence_penalty=1.5\n        ),\n        use_tqdm=True\n    )\n    choice_to_type_map = {'A': 'Primary', 'B': 'Secondary', 'C': None}\n\n    responses = [output.outputs[0].text.strip() for output in outputs]\n    \n    parsed_doi_choices = [parse_answer_with_regex(resp) for resp in responses]\n    answers2 = [choice_to_type_map.get(choice) for choice in parsed_doi_choices]\n\nelse:\n    for chunk, acc_id in zip(chunks2, ids):\n        article_id, academic_text = chunk\n        messages = [\n            {\"role\": \"system\", \"content\": SYS_PROMPT_ACCESSION},\n            {\"role\": \"user\", \"content\": f\"Accession ID: {acc_id}\\n\\nAcademic text:\\n{academic_text}\"}\n        ]\n    \n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,enable_thinking=False\n        )\n        prompts.append(prompt)\n    \n    outputs = llm.generate(\n        prompts,\n        vllm.SamplingParams(\n            seed=777,\n            temperature=0.05,\n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[mclp],\n            logprobs=len(mclp.choices)\n        ),\n        use_tqdm=True\n    )\n    \n    logprobs2 = []\n    for lps in [output.outputs[0].logprobs[0].values() for output in outputs]:\n        logprobs2.append({lp.decoded_token: lp.logprob for lp in list(lps)})\n    \n    logit_matrix2 = pd.DataFrame(logprobs2)[[\"A\", \"B\", \"C\"]].values\n    choices2 = [\"Primary\", \"Secondary\", None]\n    \n    answers2 = []\n    for logit_row in logit_matrix2:\n        max_logit = np.max(logit_row)\n        max_idx = np.argmax(logit_row)\n        \n        if max_logit > -2.0:\n            answers2.append(choices2[max_idx])\n        else:\n            answers2.append(None)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare Submission","metadata":{}},{"cell_type":"code","source":"\nsub_df = pd.DataFrame()\nsub_df[\"article_id\"] = [c[0] for c in chunks]\nsub_df[\"dataset_id\"] = doi_urls\nsub_df[\"dataset_id\"] = sub_df[\"dataset_id\"].str.lower()\nsub_df[\"type\"] = answers\nsub_df = sub_df[sub_df[\"type\"].notnull()].reset_index(drop=True)\n\nsub_df2 = pd.DataFrame()\nsub_df2[\"article_id\"] = [c[0] for c in chunks2]\nsub_df2[\"dataset_id\"] = ids\nsub_df2[\"type\"] = answers2\nsub_df2 = sub_df2[sub_df2[\"type\"].notnull()].reset_index(drop=True)\n\n# Combine and clean\nsub_df = pd.concat([sub_df, sub_df2], ignore_index=True)\nsub_df = sub_df[sub_df[\"type\"].isin([\"Primary\", \"Secondary\"])].reset_index(drop=True)\n\n# Enhanced deduplication with priority to Primary data\nsub_df = sub_df.sort_values(by=[\"article_id\", \"dataset_id\", \"type\"], \n                           key=lambda x: x.map({\"Primary\": 0, \"Secondary\": 1}) if x.name == \"type\" else x)\\\n               .drop_duplicates(subset=['article_id', 'dataset_id'], keep=\"first\")\\\n               .reset_index(drop=True)\n\nsub_df['row_id'] = range(len(sub_df))\nsub_df.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"article_id\", \"dataset_id\", \"type\"])\n\nprint(\"Final submission stats:\")\nprint(sub_df[\"type\"].value_counts())\nprint(f\"Total entries: {len(sub_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T07:18:54.42823Z","iopub.execute_input":"2025-07-08T07:18:54.429007Z","iopub.status.idle":"2025-07-08T07:18:54.452734Z","shell.execute_reply.started":"2025-07-08T07:18:54.428983Z","shell.execute_reply":"2025-07-08T07:18:54.452046Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate validation score","metadata":{}},{"cell_type":"code","source":"def f1_score(tp, fp, fn):\n    return 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n    \nif not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    pred_df = pd.read_csv(\"submission.csv\")\n    label_df = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n    label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n\n    hits_df = label_df.merge(pred_df, on=[\"article_id\", \"dataset_id\", \"type\"])\n    \n    tp = hits_df.shape[0]\n    fp = pred_df.shape[0] - tp\n    fn = label_df.shape[0] - tp\n    \n    print(\"\\nValidation Results:\")\n    print(\"TP:\", tp)\n    print(\"FP:\", fp)\n    print(\"FN:\", fn)\n    print(\"F1 Score:\", round(f1_score(tp, fp, fn), 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T07:18:54.453827Z","iopub.execute_input":"2025-07-08T07:18:54.454281Z","iopub.status.idle":"2025-07-08T07:18:54.481394Z","shell.execute_reply.started":"2025-07-08T07:18:54.454265Z","shell.execute_reply":"2025-07-08T07:18:54.480832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\ndef calculate_f1_score(y_true, y_pred):\n    if y_true.empty or y_pred.empty:\n        tp = 0\n        fp = len(y_pred)\n        fn = len(y_true)\n    else:\n        hits = y_true.merge(y_pred, on=[\"article_id\", \"dataset_id\", \"type\"])\n        tp = len(hits)\n        fp = len(y_pred) - tp\n        fn = len(y_true) - tp\n    \n    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0.0\n    return tp, fp, fn, f1\n\ndef analyze_error_sources(pred_df, label_df):\n    label_df_filtered = label_df[label_df['type'] != 'Missing'].copy()\n\n    is_doi_pred = pred_df['dataset_id'].str.startswith('https://doi.org/')\n    is_doi_label = label_df_filtered['dataset_id'].str.startswith('10.')\n\n    pred_doi = pred_df[is_doi_pred]\n    pred_accession = pred_df[~is_doi_pred]\n    label_df_filtered['dataset_id_normalized'] = label_df_filtered['dataset_id'].apply(\n        lambda x: f\"https://doi.org/{x}\" if x.startswith('10.') else x\n    )\n    label_df_filtered = label_df_filtered.rename(columns={'dataset_id': 'original_dataset_id', 'dataset_id_normalized': 'dataset_id'})\n    \n    is_doi_label_norm = label_df_filtered['dataset_id'].str.startswith('https://doi.org/')\n\n    label_doi = label_df_filtered[is_doi_label_norm]\n    label_accession = label_df_filtered[~is_doi_label_norm]\n\n    tp_doi, fp_doi, fn_doi, f1_doi = calculate_f1_score(label_doi, pred_doi)\n    tp_acc, fp_acc, fn_acc, f1_acc = calculate_f1_score(label_accession, pred_accession)\n    \n    print(\"=\"*40)\n    print(\"üî¨ Error Analysis by ID Type\")\n    print(\"=\"*40)\n\n    print(\"\\n--- DOI ---\")\n    print(f\"Total Predictions: {len(pred_doi)}\")\n    print(f\"True Positives (TP): {tp_doi}\")\n    print(f\"False Positives (FP): {fp_doi}\")\n    print(f\"False Negatives (FN): {fn_doi}\")\n    print(f\"F1 Score: {f1_doi:.4f}\")\n\n    print(\"\\n--- Accession ID ---\")\n    print(f\"Total Predictions: {len(pred_accession)}\")\n    print(f\"True Positives (TP): {tp_acc}\")\n    print(f\"False Positives (FP): {fp_acc}\")\n    print(f\"False Negatives (FN): {fn_acc}\")\n    print(f\"F1 Score: {f1_acc:.4f}\")\n    \n    print(\"\\n\" + \"=\"*40)\n    print(\"Total FP:\", fp_doi + fp_acc)\n    print(\"Total FN:\", fn_doi + fn_acc)\n    print(\"=\"*40)\n\nif not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    try:\n        pred_df = pd.read_csv(\"submission.csv\")\n        pred_df['dataset_id'] = pred_df['dataset_id'].astype(str)\n        \n        label_df = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n        label_df['dataset_id'] = label_df['dataset_id'].astype(str)\n\n        analyze_error_sources(pred_df, label_df)\n\n    except FileNotFoundError as e:\n        print(f\"Error: Could not find a required file. {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntry:\n    pred_df = pd.read_csv(\"submission.csv\")\n    label_df = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n    label_df_filtered = label_df[label_df['type'] != 'Missing'].copy()\nexcept FileNotFoundError as e:\n    print(f\"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå - {e}\")\n    exit()\n\nfn_df = pd.merge(\n    label_df_filtered,\n    pred_df,\n    on=['article_id', 'dataset_id', 'type'],\n    how='left',\n    indicator=True\n).query('_merge == \"left_only\"').drop(columns=['_merge'])\n\nmerged_df = pd.merge(\n    fn_df,\n    pred_df,\n    on=['article_id', 'dataset_id'],\n    how='left',\n    indicator='source'\n)\n\nclassified_incorrectly_df = merged_df[merged_df['source'] == 'both']\nclassified_incorrectly_count = len(classified_incorrectly_df)\n\ncompletely_missed_df = merged_df[merged_df['source'] == 'left_only']\ncompletely_missed_count = len(completely_missed_df)\n\nincorrect_doi_count = classified_incorrectly_df[classified_incorrectly_df['dataset_id'].str.startswith('https://', na=False)].shape[0]\nincorrect_accession_count = classified_incorrectly_df[~classified_incorrectly_df['dataset_id'].str.startswith('https://', na=False)].shape[0]\n\n\nmissed_doi_count = completely_missed_df[completely_missed_df['dataset_id'].str.startswith('https://', na=False)].shape[0]\nmissed_accession_count = completely_missed_df[~completely_missed_df['dataset_id'].str.startswith('https://', na=False)].shape[0]\n\n\nprint(\"=\"*55)\nprint(\"Analyst False Negatives (FN)\")\nprint(\"=\"*55)\nprint(f\"All FN: {fn_df.shape[0]} record\")\nprint(\"-\" * 55)\nprint(f\"‚Ü≥ It have but wrong answer: {classified_incorrectly_count} record\")\nprint(f\"    Wrong DOI: {incorrect_doi_count} record\")\nprint(f\"    Wrong Accession ID: {incorrect_accession_count} record\")\nprint(\"-\" * 55)\nprint(f\"‚Ü≥ Can't find this: {completely_missed_count} record\")\nprint(f\"    Can't find DOI: {missed_doi_count} record\")\nprint(f\"    Can't find Accession ID: {missed_accession_count} record\")\nprint(\"=\"*55)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}