{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"sourceType":"competition"},{"sourceId":248118764,"sourceType":"kernelVersion"},{"sourceId":166368,"sourceType":"modelInstanceVersion","modelInstanceId":141565,"modelId":164048}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Original Notebook:\n\nhttps://www.kaggle.com/code/mccocoful/fork-of-notebooke04a14364a","metadata":{}},{"cell_type":"markdown","source":"## PRE UPDATE\n\n| submit | hit | CV | LB |\n| --- | --- | --- | --- |\n| all | 0.6051 | 0.5052 | 0.479 |\n| doi | 0.2954 | 0.2309 | 0.158|\n| acc | 0.5712 | 0.4937 | 0.437|\n\n## POST UPDATE\n\n| submit | hit | CV | LB |\n| --- | --- | --- | --- |\n| all | 0.6010 | 0.5031 | 0.526 |\n| doi | 0.2874 | 0.2258 | 0.166|\n| acc | 0.5447 | 0.4708 | 0.466|\n\n## POST UPDATE + LLM\n\n| submit | hit | CV | LB |\n| --- | --- | --- | --- |\n| all | 0.6867 | 0.5739 | 0. |","metadata":{}},{"cell_type":"code","source":"! uv pip uninstall --system 'tensorflow'\n! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf' 'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'\n! mkdir -p /tmp/src","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T23:15:12.315204Z","iopub.execute_input":"2025-07-21T23:15:12.315498Z","iopub.status.idle":"2025-07-21T23:15:42.947312Z","shell.execute_reply.started":"2025-07-21T23:15:12.315471Z","shell.execute_reply":"2025-07-21T23:15:42.946557Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 5.01s\u001b[0m\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.18.0\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m157 packages\u001b[0m \u001b[2min 474ms\u001b[0m\u001b[0m                                       \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m52 packages\u001b[0m \u001b[2min 23.57s\u001b[0m\u001b[0m                                           \n\u001b[2mUninstalled \u001b[1m14 packages\u001b[0m \u001b[2min 266ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m52 packages\u001b[0m \u001b[2min 253ms\u001b[0m\u001b[0m                              \u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250622\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.9.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.7\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlogits-processor-zoo\u001b[0m\u001b[2m==0.1.12\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.6.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.26.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.47b0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions-ai\u001b[0m\u001b[2m==0.4.9\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpymupdf\u001b[0m\u001b[2m==1.26.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==24.0.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.14.7\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.8.5.post1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.29.post2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.18\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile /tmp/src/helpers.py\nimport logging, os, kagglehub, inspect\nfrom pathlib import Path\nimport polars as pl\n\nIS_KAGGLE_ENV = sum(['KAGGLE' in k for k in os.environ]) > 0\nIS_KAGGLE_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\nCOMP_DIR = Path(('/kaggle/input/make-data-count-finding-data-references' if IS_KAGGLE_SUBMISSION else kagglehub.competition_download('make-data-count-finding-data-references')))\nPDF_DIR = COMP_DIR / ('test' if IS_KAGGLE_SUBMISSION else 'train') / 'PDF'\nWORKING_DIR = Path(('/kaggle/working/' if IS_KAGGLE_ENV else '.working/'))\n\nDOI_LINK = 'https://doi.org/'\n\n# === Logging ===\nDEFAULT_LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"DEBUG\").upper() if not IS_KAGGLE_SUBMISSION else \"WARNING\"\nLOG_FILE_PATH = os.getenv(\"LOG_FILE\", \"logs/project.log\")\nLOG_DIR = Path(LOG_FILE_PATH).parent\n\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\nLOG_FORMAT = \"%(levelname)s %(asctime)s  [%(filename)s:%(lineno)d - %(funcName)s()] %(message)s\"\nLOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\n\ndef get_logger(name=None):\n    \"\"\"\n    Returns a logger named after the caller's module if no name is provided.\n    \"\"\"\n    if name is None:\n        frame = inspect.currentframe()\n        if frame is None or frame.f_back is None:\n            name = \"__main__\"\n        else:\n            name = frame.f_back.f_globals.get(\"__name__\", \"__main__\")\n\n    logger = logging.getLogger(name)\n\n    if not logger.handlers:\n        logger.setLevel(DEFAULT_LOG_LEVEL)\n\n        formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=LOG_DATEFMT)\n\n        # Console handler\n        ch = logging.StreamHandler()\n        ch.setLevel(DEFAULT_LOG_LEVEL)\n        ch.setFormatter(formatter)\n\n        # File handler\n        fh = logging.FileHandler(LOG_FILE_PATH)\n        fh.setLevel(DEFAULT_LOG_LEVEL)\n        fh.setFormatter(formatter)\n\n        logger.addHandler(ch)\n        logger.addHandler(fh)\n\n        # Prevent logs from propagating to root if not needed\n        logger.propagate = False\n    return logger\n\n# === Polars Common Expr ===\ndef is_doi_link(name:str) -> pl.Expr: return pl.col(name).str.starts_with(DOI_LINK)\ndef string_normalization(name:str) -> pl.Expr: return pl.col(name).str.normalize(\"NFKC\").str.replace_all(r\"[^\\p{Ascii}]\", '').str.replace_all(r\"https?://zenodo\\.org/record/(\\d+)\", r\" 10.5281/zenodo.$1 \")\n\ndef get_df(parse_dir:str):\n    records = []\n    txt_files = list(Path(parse_dir).glob('*.txt'))\n    for txt_file in txt_files:\n        id_ = txt_file.stem\n        with open(txt_file, 'r') as f: text = f.read()\n        records.append({'article_id': id_, 'text': text})\n    return pl.DataFrame(records).with_columns(string_normalization('text').alias('text'))\n\ndef assume_type(df: pl.DataFrame) -> pl.DataFrame:\n    return (\n        df.with_columns(pl.when(is_doi_link('dataset_id').or_(pl.col('dataset_id').str.starts_with('SAMN'))).then(pl.lit('Primary')).otherwise(pl.lit('Secondary')).alias('type'))\n    )\n\n\ndef score(df, gt, on, tag='all'):\n    hits = gt.join(df, on=on)\n    tp = hits.height\n    fp = df.height - tp\n    fn = gt.height - tp\n\n    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n\n    return f\"{tag} - f1: {f1:.4f} [{tp}/{fp}/{fn}]\"\n\ndef evaluate(df, on=['article_id', 'dataset_id']):\n    gt = pl.read_csv(COMP_DIR/'train_labels.csv').filter(pl.col('type')!='Missing')\n\n    return (\n        score(df, gt, on),\n        score(df.filter(is_doi_link('dataset_id')), gt.filter(is_doi_link('dataset_id')), on, 'doi'),\n        score(df.filter(~is_doi_link('dataset_id')), gt.filter(~is_doi_link('dataset_id')), on, 'acc'),\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T23:15:42.949483Z","iopub.execute_input":"2025-07-21T23:15:42.949702Z","iopub.status.idle":"2025-07-21T23:15:42.957842Z","shell.execute_reply.started":"2025-07-21T23:15:42.949669Z","shell.execute_reply":"2025-07-21T23:15:42.957187Z"}},"outputs":[{"name":"stdout","text":"Writing /tmp/src/helpers.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile /tmp/src/parse.py\nimport argparse\nfrom pathlib import Path\nimport pymupdf\n\nfrom helpers import get_logger, PDF_DIR\n\nl = get_logger()\n\ndef pdf_to_txt(output_dir: Path):\n    output_dir.mkdir(parents=True, exist_ok=True)\n    pdf_files = list(PDF_DIR.glob(\"*.pdf\")) + list(PDF_DIR.glob(\"*.PDF\"))\n    existing_txt_files = {f.stem for f in output_dir.glob(\"*.txt\")}\n    for pdf_file in pdf_files:\n        txt_file = output_dir / f\"{pdf_file.stem}.txt\"\n        if pdf_file.stem in existing_txt_files:\n            l.debug(f\"Skipping {pdf_file.name} - {txt_file.name} already exists\")\n            continue\n        l.debug(f\"Processing {pdf_file.name}...\")\n        try:\n            text = \"\"\n            with pymupdf.open(pdf_file) as doc:\n                for page in doc: text += page.get_text()\n            \n            txt_file.write_text(text, encoding='utf-8')\n            l.debug(f\"Successfully created {txt_file.name}\")\n        except Exception as e:\n            l.warning(f\"Error processing {pdf_file.name}: {str(e)}\")\n\ndef main():    \n    parser = argparse.ArgumentParser(description='Convert PDF files to text files.')\n    parser.add_argument('output_dir', type=Path, help='Directory to save text files')\n    args = parser.parse_args()\n    l.info('PARSE START')\n    pdf_to_txt(args.output_dir)\n    l.info('PARSE FINISH')\n\nif __name__ == \"__main__\": main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T23:15:42.958703Z","iopub.execute_input":"2025-07-21T23:15:42.959133Z","iopub.status.idle":"2025-07-21T23:15:43.207155Z","shell.execute_reply.started":"2025-07-21T23:15:42.959100Z","shell.execute_reply":"2025-07-21T23:15:43.206409Z"}},"outputs":[{"name":"stdout","text":"Writing /tmp/src/parse.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile /tmp/src/check_parse.py\nimport polars as pl\nfrom pathlib import Path\n\nfrom helpers import *\n\nl=get_logger()\n\ndef gt_dataset_id_normalization(name:str) -> pl.Expr:\n    return (\n        pl.when(is_doi_link(name))\n        .then(pl.col(name).str.split(DOI_LINK).list.last())\n        .otherwise(name)\n        .str.to_lowercase()\n    )\n\ndef main():\n    if IS_KAGGLE_SUBMISSION:\n        l.debug('skipping check_parse for submission')\n        return\n    df = (\n        get_df('/tmp/train_parse')\n        .with_columns(pl.col('text').str.replace_all('\\s+', '').str.to_lowercase().alias('text'))\n    )\n\n    gt = (\n        pl.read_csv(COMP_DIR/'train_labels.csv')\n        .filter(pl.col('article_id').is_in(df['article_id']))\n        .filter(pl.col('type')!='Missing')\n        .with_columns(gt_dataset_id_normalization('dataset_id').alias('norm_id'))\n    )\n\n    l.info(f\"pymupdf misses: {gt.join(df, on='article_id').with_columns(hit=pl.col('text').str.contains(pl.col('norm_id'), literal=True)).filter(~pl.col('hit')).height} dataset_ids\")\n\nif __name__=='__main__': main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T23:15:43.208039Z","iopub.execute_input":"2025-07-21T23:15:43.208351Z","iopub.status.idle":"2025-07-21T23:15:43.236551Z","shell.execute_reply.started":"2025-07-21T23:15:43.208325Z","shell.execute_reply":"2025-07-21T23:15:43.235864Z"}},"outputs":[{"name":"stdout","text":"Writing /tmp/src/check_parse.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile /tmp/src/getid.py\nimport re\nimport polars as pl\nfrom pathlib import Path\nfrom typing import Optional, Tuple\n\nfrom helpers import *\n\nCOMPILED_PATTERNS = {\n'ref_header_patterns': [re.compile(r'\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE CITED|WORKS CITED|CITED WORKS|ACKNOWLEDGEMENTS)\\b[:\\s]*', re.IGNORECASE)],\n'ref_header_patterns1': [re.compile(r'(?<!\\w)[*_]{0,3}\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE\\s+CITED|WORKS\\s+CITED|CITED\\s+WORKS)\\b[*_]{0,3}(?!\\w)[:\\s]*', re.I)],\n'citation_pattern': re.compile(r'^\\s*(\\[\\d+\\]|\\(\\d+\\)|\\d+\\.|\\d+\\)|\\d+(?=\\s|$))\\s*'),\n'first_citation_patterns': [re.compile(r'^\\s*\\[1\\]\\s*'), re.compile(r'^\\s*\\(1\\)\\s*'), re.compile(r'^\\s*1\\.\\s*'), re.compile(r'^\\s*1\\)\\s*'), re.compile(r'^\\s*1(?=\\s|$)')],\n}\n\nl = get_logger()\n\ndef find_last_reference_header(text: str, header_patterns: list[re.Pattern]) -> Optional[int]:\n    last_match_idx = None\n    for pattern in header_patterns:\n        matches = list(pattern.finditer(text))\n        if matches:\n            last_match_idx = matches[-1].start()\n    return last_match_idx\n\ndef find_last_first_citation(text: str) -> Optional[int]:\n    lines = text.splitlines()\n    last_match_line = None\n\n    for line_num, line in enumerate(lines):\n        line = line.strip()\n        for pattern in COMPILED_PATTERNS['first_citation_patterns']:\n            if pattern.match(line):\n                next_lines = lines[line_num:line_num+3]\n                if any(COMPILED_PATTERNS['citation_pattern'].match(l.strip()) for l in next_lines[1:]):\n                    last_match_line = line_num\n                break\n    return last_match_line\n\ndef find_reference_start(text: str) -> Optional[int]:\n    lines = text.splitlines()\n\n    last_first_citation = find_last_first_citation(text)\n    if last_first_citation is not None:\n        return last_first_citation\n\n    start_search_idx = int(len(lines) * 0.5)  # Start from middle\n    for i in range(start_search_idx, len(lines)):\n        line = lines[i].strip()\n        if COMPILED_PATTERNS['citation_pattern'].match(line):\n            next_lines = lines[i:i+3]\n            if sum(1 for l in next_lines if COMPILED_PATTERNS['citation_pattern'].match(l.strip())) >= 2:\n                for j in range(i, max(-1, i-10), -1):\n                    if not COMPILED_PATTERNS['citation_pattern'].match(lines[j].strip()):\n                        return j + 1\n                return max(0, i-10)\n\n    return None\n\ndef split_text_and_references(text: str) -> Tuple[str, str]:\n    header_idx = find_last_reference_header(text, COMPILED_PATTERNS['ref_header_patterns'])\n    if header_idx is not None:\n        header_idx2 = find_last_reference_header(text[:header_idx].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n        if header_idx2 is not None:\n            header_idx3 = find_last_reference_header(text[:header_idx2].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n            if header_idx3 is not None:\n                return text[:header_idx3].strip(), text[header_idx3:].strip()\n            return text[:header_idx2].strip(), text[header_idx2:].strip()\n        return text[:header_idx].strip(), text[header_idx:].strip()\n\n    ref_start_line = find_reference_start(text)\n    if ref_start_line is not None:\n        lines = text.splitlines()\n        body = '\\n'.join(lines[:ref_start_line])\n        refs = '\\n'.join(lines[ref_start_line:])\n        return body.strip(), refs.strip()\n\n    return text.strip(), ''\n\ndef get_splits(df):\n    main_texts, ref_texts = [], []\n    for raw_text in df['text']:\n        main, refs = split_text_and_references(raw_text)\n        main_texts.append(main)\n        ref_texts.append(refs)\n    df = df.with_columns(pl.Series('body', main_texts), pl.Series('ref', ref_texts))\n    return df\n\ndef tidy_extraction(df) -> pl.DataFrame:\n    bad_ids = [f'{DOI_LINK}{e}' for e in ['10.5061/dryad', '10.5281/zenodo', '10.6073/pasta']]\n    doi_df = (\n        df\n        .with_columns(pl.col('body').str.extract_all(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+').alias('match'))\n        .explode('match')\n        .drop_nulls('match')\n        .with_columns(pl.col('match').str.replace_all(r'\\s+', '').str.replace(r'[^A-Za-z0-9]+$', '').str.to_lowercase().alias('dataset_id'))\n        .group_by('article_id','dataset_id').agg('match')\n        .with_columns((DOI_LINK+pl.col('dataset_id')).alias('dataset_id'))\n    )\n\n\n    acc_df = (\n        df\n        .with_columns(pl.col('text').str.extract_all(r'(?i)\\b(?:CHEMBL\\d+|E-GEOD-\\d+|E-PROT-\\d+|EMPIAR-\\d+|ENSBTAG\\d+|ENSOARG\\d+|EPI_ISL_\\d{5,}|EPI\\d{6,7}|HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|KX\\d{6}|K0\\d{4}|PRJNA\\d+|PXD\\d+|SAMN\\d+|dryad\\s*\\.\\s*[^\\s\"<>]+|pasta\\s*/\\s*[^\\s\"<>])').alias('match'))\n        .explode('match')\n        .drop_nulls('match')\n        .with_columns(pl.col('match').str.replace_all(r'\\s+', '').str.replace(r'[^A-Za-z0-9]+$', '').alias('dataset_id'))\n        .group_by('article_id','dataset_id').agg('match')\n        .with_columns(pl.when(pl.col('dataset_id').str.starts_with('dryad.')).then(f'{DOI_LINK}10.5061/' + pl.col('dataset_id')).otherwise('dataset_id').alias('dataset_id'))\n        .with_columns(pl.when(pl.col('dataset_id').str.starts_with('pasta/')).then(f'{DOI_LINK}10.6073/' + pl.col('dataset_id')).otherwise('dataset_id').alias('dataset_id'))\n    )\n\n    df = pl.concat([doi_df, acc_df])\n    \n    df = (\n        df\n        .unique('dataset_id')\n        .filter(~pl.col('article_id').str.replace('_','/').str.contains(pl.col('dataset_id').str.split(DOI_LINK).list.last().str.escape_regex()))\n        .filter(~pl.col('dataset_id').str.contains(pl.col('article_id').str.replace('_','/').str.escape_regex()))\n        .filter(~pl.col('dataset_id').str.contains('figshare', literal=True))\n        .filter(~pl.col('dataset_id').is_in(bad_ids))\n        .filter(pl.when(is_doi_link('dataset_id').and_(pl.col('dataset_id').str.split('/').list.last().str.len_chars()<5)).then(False).otherwise(True))\n        .with_columns(pl.col('match').list.unique())\n    )\n    return df\n\ndef get_context_window(text: str, substring: str, window: int = 100) -> str:\n    index = text.find(substring)\n    if index == -1:\n        raise Error\n\n    start = max(index - window, 0)\n    end = min(index + len(substring) + window, len(text))\n    return text[start:end]\n\ndef get_window_df(text_df, ids_df):\n    df = ids_df.join(text_df, on='article_id')\n    windows = []\n    for text, match_ids in df.select('text', 'match').rows():\n        windows.append(get_context_window(text, match_ids[0]))\n        \n    return df.with_columns(pl.Series('window', windows)).select('article_id', 'dataset_id', 'window')\n\n\ndef main():\n    text_df = get_df('/tmp/train_parse')\n    df = get_splits(text_df)\n    df = tidy_extraction(df)\n    df = get_window_df(text_df, df)\n    df.write_parquet('/tmp/extracted.parquet')\n    \n    df = assume_type(df)\n    \n    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n    l.info(\"submission saved...\")\n\n    if not IS_KAGGLE_SUBMISSION:\n        l.info(f\"total unique citations: {df.height}\")\n        l.info('### HITS ###')\n        results = evaluate(df)\n        for r in results: l.info(r) \n        l.info('### TYPES ###')\n        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n        for r in results: l.info(r) \n\n\nif __name__=='__main__': main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T23:15:43.237382Z","iopub.execute_input":"2025-07-21T23:15:43.237612Z","iopub.status.idle":"2025-07-21T23:15:43.262184Z","shell.execute_reply.started":"2025-07-21T23:15:43.237594Z","shell.execute_reply":"2025-07-21T23:15:43.261428Z"}},"outputs":[{"name":"stdout","text":"Writing /tmp/src/getid.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile /tmp/src/llm_validate.py\nimport polars as pl\nimport os\nimport argparse\n\nfrom helpers import *\n\nl = get_logger()\n\nSYS_PROMPT_CLASSIFY_DOI = \"\"\"\nYou are give a piece of academic text. Your task is to is to classify whether the DOI citation refers specifically to open-access research data.\nClassify the DOI as:\nA) Data: if the DOI is related to a dataset.\nB) Literature: does not refer to research data or is a written academic paper.\n\nRespond with only one letter: A or B.\n\"\"\".strip()\n\n\ndef build_df():\n    df = pl.read_parquet('/tmp/extracted.parquet')\n    df.filter(~is_doi_link('dataset_id')).select('article_id', 'dataset_id').write_csv('/tmp/accid_sub.csv')\n    df = df.filter(is_doi_link('dataset_id'))\n    return df\n\ndef build_prompt(tokenizer, df):\n    prompts = []\n    for doi, text in df.select('dataset_id', 'window').rows():\n        messages = [{'role':'system','content': SYS_PROMPT_CLASSIFY_DOI}, {'role':'user', 'content': text}]\n        prompts.append(tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False))\n    df = df.with_columns(pl.Series('prompt', prompts))\n    return df\n\nif __name__=='__main__':\n    os.environ[\"VLLM_USE_V1\"] = \"0\"\n    l.info('importing vllm...')\n    import vllm\n    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n    model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n\n    l.info('loading vllm...')\n    \n    llm = vllm.LLM(\n        model_path,\n        quantization='awq',\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2048,\n        disable_log_stats=True,\n        disable_custom_all_reduce=True,\n        enable_prefix_caching=True,\n        task='generate',\n    )\n    l.info('building prompts...')\n    \n    tokenizer = llm.get_tokenizer()\n    df = build_df()\n    df = build_prompt(tokenizer, df)\n    prompts = df['prompt'].to_list()\n    l.info('generation start')\n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0.1, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n    choices = [max(d, key=d.get) for d in logprobs]\n    types = {'A': True, 'B': False}\n    choices = [types[c] for c in choices]\n    \n    l.info('generation end')\n    df = df.with_columns(pl.Series('type', choices))\n    print(df.group_by('type').agg(pl.len()))\n    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n    df = pl.concat([\n        pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')\n    ])\n\n    df = assume_type(df)\n    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n    \n\n    if not IS_KAGGLE_SUBMISSION:\n        l.info(f\"total unique citations: {df.height}\")\n        l.info('### HITS ###')\n        results = evaluate(df)\n        for r in results: l.info(r) \n        l.info('### TYPES ###')\n        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n        for r in results: l.info(r) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T23:15:43.262895Z","iopub.execute_input":"2025-07-21T23:15:43.263099Z","iopub.status.idle":"2025-07-21T23:15:43.285984Z","shell.execute_reply.started":"2025-07-21T23:15:43.263082Z","shell.execute_reply":"2025-07-21T23:15:43.285172Z"}},"outputs":[{"name":"stdout","text":"Writing /tmp/src/llm_validate.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%cd /tmp\n!LOG_LEVEL=INFO python src/parse.py /tmp/train_parse\n! python src/check_parse.py\n! python src/getid.py\n! python src/llm_validate.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T23:15:43.287806Z","iopub.execute_input":"2025-07-21T23:15:43.288290Z","iopub.status.idle":"2025-07-21T23:24:40.923202Z","shell.execute_reply.started":"2025-07-21T23:15:43.288261Z","shell.execute_reply":"2025-07-21T23:24:40.922265Z"},"scrolled":true,"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"/tmp\nINFO 2025-07-21 23:15:46  [parse.py:33 - main()] PARSE START\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nINFO 2025-07-21 23:18:01  [parse.py:35 - main()] PARSE FINISH\nINFO 2025-07-21 23:18:05  [check_parse.py:32 - main()] pymupdf misses: 42 dataset_ids\nINFO 2025-07-21 23:18:11  [getid.py:154 - main()] submission saved...\nINFO 2025-07-21 23:18:11  [getid.py:157 - main()] total unique citations: 873\nINFO 2025-07-21 23:18:11  [getid.py:158 - main()] ### HITS ###\nINFO 2025-07-21 23:18:11  [getid.py:160 - main()] all - f1: 0.6043 [481/392/238]\nINFO 2025-07-21 23:18:11  [getid.py:160 - main()] doi - f1: 0.4356 [164/264/161]\nINFO 2025-07-21 23:18:11  [getid.py:160 - main()] acc - f1: 0.7557 [317/128/77]\nINFO 2025-07-21 23:18:11  [getid.py:161 - main()] ### TYPES ###\nINFO 2025-07-21 23:18:11  [getid.py:163 - main()] all - f1: 0.5050 [402/471/317]\nINFO 2025-07-21 23:18:11  [getid.py:163 - main()] doi - f1: 0.3400 [128/300/197]\nINFO 2025-07-21 23:18:11  [getid.py:163 - main()] acc - f1: 0.6532 [274/171/120]\nINFO 2025-07-21 23:18:13  [llm_validate.py:35 - <module>()] importing vllm...\nINFO 07-21 23:18:31 [__init__.py:239] Automatically detected platform cuda.\nINFO 2025-07-21 23:18:37  [llm_validate.py:40 - <module>()] loading vllm...\nWARNING 07-21 23:18:50 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 07-21 23:18:50 [config.py:1770] Defaulting to use mp for distributed inference\nWARNING 07-21 23:18:50 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nINFO 07-21 23:18:50 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \nWARNING 07-21 23:18:51 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:18:51 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\nINFO 07-21 23:18:51 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 07-21 23:18:51 [cuda.py:289] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:18:51 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:18:51 [cuda.py:289] Using XFormers backend.\n[W721 23:19:02.137243673 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W721 23:19:02.492056220 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W721 23:19:12.144128115 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W721 23:19:22.154319284 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\nINFO 07-21 23:19:22 [utils.py:1055] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:19:22 [utils.py:1055] Found nccl from library libnccl.so.2\nINFO 07-21 23:19:22 [pynccl.py:69] vLLM is using nccl==2.21.5\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:19:22 [pynccl.py:69] vLLM is using nccl==2.21.5\nINFO 07-21 23:19:23 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_7bc327d6'), local_subscribe_addr='ipc:///tmp/2d5213a6-3b8c-4eb0-9398-78d639c3480d', remote_subscribe_addr=None, remote_addr_ipv6=False)\nINFO 07-21 23:19:23 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:19:23 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\nINFO 07-21 23:19:23 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:19:23 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:32<02:09, 32.40s/it]\nLoading safetensors checkpoint shards:  40% Completed | 2/5 [01:07<01:42, 34.12s/it]\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [01:54<01:19, 39.76s/it]\nLoading safetensors checkpoint shards:  80% Completed | 4/5 [02:41<00:42, 42.56s/it]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [03:27<00:00, 43.90s/it]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [03:27<00:00, 41.46s/it]\n\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:22:51 [loader.py:458] Loading weights took 207.40 seconds\nINFO 07-21 23:22:51 [loader.py:458] Loading weights took 207.66 seconds\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:22:51 [model_runner.py:1140] Model loading took 9.0935 GiB and 207.959719 seconds\nINFO 07-21 23:22:51 [model_runner.py:1140] Model loading took 9.0935 GiB and 208.224015 seconds\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:23:06 [worker.py:287] Memory profiling takes 14.57 seconds\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:23:06 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n\u001b[1;36m(VllmWorkerProcess pid=189)\u001b[0;0m INFO 07-21 23:23:06 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 3.63GiB.\nINFO 07-21 23:23:06 [worker.py:287] Memory profiling takes 14.81 seconds\nINFO 07-21 23:23:06 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\nINFO 07-21 23:23:06 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 2.66GiB.\nINFO 07-21 23:23:07 [executor_base.py:112] # cuda blocks: 1363, # CPU blocks: 2048\nINFO 07-21 23:23:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 10.65x\nINFO 07-21 23:23:11 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 19.41 seconds\nINFO 2025-07-21 23:23:11  [llm_validate.py:56 - <module>()] building prompts...\nINFO 2025-07-21 23:23:11  [llm_validate.py:62 - <module>()] generation start\nProcessed prompts: 100%|█| 428/428 [01:21<00:00,  5.23it/s, est. speed input: 90\nINFO 2025-07-21 23:24:34  [llm_validate.py:70 - <module>()] generation end\nshape: (2, 2)\n┌───────┬─────┐\n│ type  ┆ len │\n│ ---   ┆ --- │\n│ bool  ┆ u32 │\n╞═══════╪═════╡\n│ true  ┆ 237 │\n│ false ┆ 191 │\n└───────┴─────┘\nINFO 2025-07-21 23:24:34  [llm_validate.py:83 - <module>()] total unique citations: 682\nINFO 2025-07-21 23:24:34  [llm_validate.py:84 - <module>()] ### HITS ###\nINFO 2025-07-21 23:24:34  [llm_validate.py:86 - <module>()] all - f1: 0.6867 [481/201/238]\nINFO 2025-07-21 23:24:34  [llm_validate.py:86 - <module>()] doi - f1: 0.5836 [164/73/161]\nINFO 2025-07-21 23:24:34  [llm_validate.py:86 - <module>()] acc - f1: 0.7557 [317/128/77]\nINFO 2025-07-21 23:24:34  [llm_validate.py:87 - <module>()] ### TYPES ###\nINFO 2025-07-21 23:24:34  [llm_validate.py:89 - <module>()] all - f1: 0.5739 [402/280/317]\nINFO 2025-07-21 23:24:34  [llm_validate.py:89 - <module>()] doi - f1: 0.4555 [128/109/197]\nINFO 2025-07-21 23:24:34  [llm_validate.py:89 - <module>()] acc - f1: 0.6532 [274/171/120]\nERROR 07-21 23:24:35 [multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 189 died, exit code: -15\nINFO 07-21 23:24:35 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\n[rank0]:[W721 23:24:36.112675490 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nException ignored in: <Finalize object, dead>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 227, in __call__\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 206, in _finalize_close\nTypeError: 'NoneType' object is not callable\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"! cat /tmp/logs/project.log","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T23:24:40.924302Z","iopub.execute_input":"2025-07-21T23:24:40.924576Z","iopub.status.idle":"2025-07-21T23:24:41.044755Z","shell.execute_reply.started":"2025-07-21T23:24:40.924549Z","shell.execute_reply":"2025-07-21T23:24:41.043841Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"INFO 2025-07-21 23:15:46  [parse.py:33 - main()] PARSE START\nINFO 2025-07-21 23:18:01  [parse.py:35 - main()] PARSE FINISH\nINFO 2025-07-21 23:18:05  [check_parse.py:32 - main()] pymupdf misses: 42 dataset_ids\nINFO 2025-07-21 23:18:11  [getid.py:154 - main()] submission saved...\nINFO 2025-07-21 23:18:11  [getid.py:157 - main()] total unique citations: 873\nINFO 2025-07-21 23:18:11  [getid.py:158 - main()] ### HITS ###\nINFO 2025-07-21 23:18:11  [getid.py:160 - main()] all - f1: 0.6043 [481/392/238]\nINFO 2025-07-21 23:18:11  [getid.py:160 - main()] doi - f1: 0.4356 [164/264/161]\nINFO 2025-07-21 23:18:11  [getid.py:160 - main()] acc - f1: 0.7557 [317/128/77]\nINFO 2025-07-21 23:18:11  [getid.py:161 - main()] ### TYPES ###\nINFO 2025-07-21 23:18:11  [getid.py:163 - main()] all - f1: 0.5050 [402/471/317]\nINFO 2025-07-21 23:18:11  [getid.py:163 - main()] doi - f1: 0.3400 [128/300/197]\nINFO 2025-07-21 23:18:11  [getid.py:163 - main()] acc - f1: 0.6532 [274/171/120]\nINFO 2025-07-21 23:18:13  [llm_validate.py:35 - <module>()] importing vllm...\nINFO 2025-07-21 23:18:37  [llm_validate.py:40 - <module>()] loading vllm...\nINFO 2025-07-21 23:23:11  [llm_validate.py:56 - <module>()] building prompts...\nINFO 2025-07-21 23:23:11  [llm_validate.py:62 - <module>()] generation start\nINFO 2025-07-21 23:24:34  [llm_validate.py:70 - <module>()] generation end\nINFO 2025-07-21 23:24:34  [llm_validate.py:83 - <module>()] total unique citations: 682\nINFO 2025-07-21 23:24:34  [llm_validate.py:84 - <module>()] ### HITS ###\nINFO 2025-07-21 23:24:34  [llm_validate.py:86 - <module>()] all - f1: 0.6867 [481/201/238]\nINFO 2025-07-21 23:24:34  [llm_validate.py:86 - <module>()] doi - f1: 0.5836 [164/73/161]\nINFO 2025-07-21 23:24:34  [llm_validate.py:86 - <module>()] acc - f1: 0.7557 [317/128/77]\nINFO 2025-07-21 23:24:34  [llm_validate.py:87 - <module>()] ### TYPES ###\nINFO 2025-07-21 23:24:34  [llm_validate.py:89 - <module>()] all - f1: 0.5739 [402/280/317]\nINFO 2025-07-21 23:24:34  [llm_validate.py:89 - <module>()] doi - f1: 0.4555 [128/109/197]\nINFO 2025-07-21 23:24:34  [llm_validate.py:89 - <module>()] acc - f1: 0.6532 [274/171/120]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Result:\n\nScore: 0.532\n\nRank: 65 (2025-07-22-8:40, JST)\n\nRuntime : 8min (kaggle editor), 20min (Scoring)\n\nYour Best Entry!\nYour most recent submission scored 0.532, which is an improvement of your previous score of 0.521. Great job!\n\nMoved up to rank 65 on #kaggle. I'm not addicted. I can quit when I want. https://kaggle.com/competitions/make-data-count-finding-data-references ","metadata":{}},{"cell_type":"markdown","source":"Make Data Count, Maggie Demkin, and Walter Reade. Make Data Count - Finding Data References. https://kaggle.com/competitions/make-data-count-finding-data-references, 2025. Kaggle.","metadata":{}}]}