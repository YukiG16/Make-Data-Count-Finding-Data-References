{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":82370,"databundleVersionId":12656064,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Original NoteBook: \n\nhttps://www.kaggle.com/code/shohaanatoliy/introduce-a-not-a-citation","metadata":{}},{"cell_type":"code","source":"# Install the required packages from the provided dependency file\n# !pip install -r /kaggle/input/make-data-count-finding-data-references/dependencies/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:09:42.471871Z","iopub.execute_input":"2025-06-15T09:09:42.472084Z","iopub.status.idle":"2025-06-15T09:09:42.476018Z","shell.execute_reply.started":"2025-06-15T09:09:42.472053Z","shell.execute_reply":"2025-06-15T09:09:42.475408Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import importlib\n\n# List of packages to check\npackages = [\"pandas\", \"numpy\", \"spacy\", \"scikit-learn\", \"joblib\", \"PyPDF2\"]\n\n# Check each package\nfor package in packages:\n    try:\n        importlib.import_module(package)\n        print(f\"✅ {package} is installed.\")\n    except ImportError:\n        print(f\"❌ {package} is NOT installed.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:09:42.476632Z","iopub.execute_input":"2025-06-15T09:09:42.476832Z","iopub.status.idle":"2025-06-15T09:09:52.107161Z","shell.execute_reply.started":"2025-06-15T09:09:42.476814Z","shell.execute_reply":"2025-06-15T09:09:52.106405Z"}},"outputs":[{"name":"stdout","text":"✅ pandas is installed.\n✅ numpy is installed.\n✅ spacy is installed.\n❌ scikit-learn is NOT installed.\n✅ joblib is installed.\n❌ PyPDF2 is NOT installed.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport re\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport xml.etree.ElementTree as ET\nfrom pathlib import Path\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n    GradientBoostingClassifier,\n    VotingClassifier,\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction import DictVectorizer\n\nwarnings.filterwarnings(\"ignore\")\n\ntry:\n    import spacy\n\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept:\n    nlp = None\n    print(\n        \"Run `python -m spacy download en_core_web_sm` to enable NLP features.\"\n    )\n\ntry:\n    import fitz  # PyMuPDF\nexcept ImportError:\n    fitz = None\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    tqdm = lambda x: x  # fallback\n\n\nclass DataCitationDetector:\n    def __init__(self):\n        self.vectorizer = DictVectorizer(sparse=False)\n        self.label_encoder = LabelEncoder()\n        self.nlp = nlp\n\n        self.classifier = VotingClassifier(\n            estimators=[\n                (\n                    \"rf\",\n                    RandomForestClassifier(\n                        n_estimators=200,\n                        max_depth=10,\n                        class_weight=\"balanced\",\n                        random_state=42,\n                    ),\n                ),\n                (\n                    \"gb\",\n                    GradientBoostingClassifier(\n                        n_estimators=100, max_depth=6, random_state=42\n                    ),\n                ),\n                (\n                    \"lr\",\n                    LogisticRegression(\n                        max_iter=1000,\n                        class_weight=\"balanced\",\n                        random_state=42,\n                    ),\n                ),\n            ],\n            voting=\"soft\",\n        )\n\n        self.primary_patterns = [\n            r\"\\b(?:we\\s+)?(?:generated|collected|produced|created|measured|recorded|obtained|acquired)\\b\",\n            r\"\\bthis\\s+study\\b\",\n            r\"\\bour\\s+(?:data|dataset|measurements|results)\\b\",\n            r\"\\bnew\\s+data\\b\",\n            r\"\\boriginal\\s+data\\b\",\n            r\"\\bnewly\\s+(?:generated|collected)\\b\",\n            r\"\\bspecifically\\s+(?:for\\s+)?this\\s+(?:study|work)\\b\",\n            r\"\\bin-house\\s+(?:generated|created)\\b\",\n        ]\n\n        self.secondary_patterns = [\n            r\"\\bobtained\\s+from\\b\",\n            r\"\\breused?\\b\",\n            r\"\\bexisting\\s+data\\b\",\n            r\"\\bpreviously\\s+(?:published|reported)\\b\",\n            r\"\\bderived\\s+from\\b\",\n            r\"\\bopen\\s+data\\b\",\n            r\"\\bthird-party\\s+data\\b\",\n            r\"\\bbenchmark\\s+data\\b\",\n        ]\n\n        self.section_patterns = {\n            \"methods\": r\"\\bmethods?\\b\",\n            \"results\": r\"\\bresults?\\b\",\n            \"introduction\": r\"\\bintroduction\\b\",\n            \"discussion\": r\"\\bdiscussion\\b\",\n            \"references\": r\"\\breferences?\\b\",\n            \"data_availability\": r\"\\bdata\\s+availability\\b\",\n        }\n\n        self.citation_patterns = {\n            \"doi\": r\"(?:https?://)?(?:dx\\.)?doi\\.org/(10\\.\\d{4,9}/[\\w./-]+)\",\n            \"zenodo\": r\"(?:https?://)?zenodo\\.org/record/(\\d+)\",\n            \"github\": r\"(?:https?://)?github\\.com/[\\w\\-_]+/[\\w\\-_]+\",\n            \"gse\": r\"GSE\\d+\",\n            \"sra\": r\"SRA\\d+\",\n            \"prjna\": r\"PRJNA\\d+\",\n            \"chembl\": r\"CHEMBL\\d+\",\n            \"pdb\": r\"PDB:\\w+\",\n            \"uniprot\": r\"UniProt:\\w+\",\n        }\n\n    def extract_text_from_xml(self, xml_path):\n        try:\n            tree = ET.parse(xml_path)\n            root = tree.getroot()\n            return \" \".join(\n                elem.text.strip() for elem in root.iter() if elem.text\n            )\n        except:\n            return \"\"\n\n    def extract_text_from_pdf(self, pdf_path):\n        try:\n            if fitz:\n                doc = fitz.open(pdf_path)\n                text = \"\".join([page.get_text() for page in doc])\n                doc.close()\n                return text\n        except:\n            return \"\"\n        return \"\"\n\n    def load_documents(self, base_path, split=\"train\"):\n        documents = {}\n        for xml_file in Path(base_path, split, \"XML\").glob(\"*.xml\"):\n            documents[xml_file.stem] = self.extract_text_from_xml(xml_file)\n        for pdf_file in Path(base_path, split, \"PDF\").glob(\"*.pdf\"):\n            if pdf_file.stem not in documents:\n                documents[pdf_file.stem] = self.extract_text_from_pdf(\n                    pdf_file\n                )\n        return documents\n\n    def extract_data_citations(self, text):\n        citations = set()\n        for name, pattern in self.citation_patterns.items():\n            for match in re.findall(pattern, text, re.IGNORECASE):\n                if name == \"doi\":\n                    # Normalize DOI by removing trailing characters that are not part of it\n                    clean_match = match.rstrip(\".,;)\")\n                    citations.add(f\"https://doi.org/{clean_match}\")\n                elif name == \"zenodo\":\n                    citations.add(f\"https://zenodo.org/record/{match}\")\n                else:\n                    citations.add(match.strip().rstrip(\".,;)\"))\n        return list(citations)\n\n    def create_features(self, text, citation):\n        features = {}\n        text_lower = text.lower()\n        # Normalize citation for matching\n        citation_lower = citation.lower().replace(\"https://doi.org/\", \"\")\n        citation_escaped = re.escape(citation_lower)\n\n        context_match = re.search(\n            f\".{{0,500}}{citation_escaped}.{{0,500}}\", text_lower\n        )\n        context = context_match.group() if context_match else text_lower\n\n        features[\"text_length\"] = len(text)\n        features[\"citation_count\"] = text_lower.count(citation_lower)\n\n        for i, pattern in enumerate(self.primary_patterns):\n            features[f\"primary_{i}\"] = len(\n                re.findall(pattern, context, re.IGNORECASE)\n            )\n        for i, pattern in enumerate(self.secondary_patterns):\n            features[f\"secondary_{i}\"] = len(\n                re.findall(pattern, context, re.IGNORECASE)\n            )\n\n        for sec, sec_pattern in self.section_patterns.items():\n            features[f\"in_{sec}\"] = int(bool(re.search(sec_pattern, context)))\n\n        features[\"is_doi\"] = int(\"doi.org\" in citation.lower())\n        features[\"is_github\"] = int(\"github.com\" in citation.lower())\n        features[\"is_database_id\"] = int(\n            any(\n                x in citation.lower()\n                for x in [\"chembl\", \"gse\", \"sra\", \"prjna\"]\n            )\n        )\n\n        features[\"near_figure\"] = len(\n            re.findall(r\"fig(?:ure)?\\s*\\d+\", context, re.IGNORECASE)\n        )\n        features[\"near_table\"] = len(\n            re.findall(r\"table\\s*\\d+\", context, re.IGNORECASE)\n        )\n        features[\"near_supplement\"] = len(\n            re.findall(r\"supplement\", context, re.IGNORECASE)\n        )\n\n        if self.nlp:\n            doc = self.nlp(context[:1000])\n            features[\"num_entities\"] = len(doc.ents)\n        else:\n            features[\"num_entities\"] = 0\n\n        return features\n\n    def train_model(self, base_path):\n        print(\"Training model...\")\n        documents = self.load_documents(base_path, \"train\")\n        labels_df = pd.read_csv(Path(base_path) / \"train_labels.csv\")\n\n        \n        # Create a lookup for true labels for faster access\n        true_labels = {}\n        for _, row in labels_df.iterrows():\n            article_id = row[\"article_id\"]\n            dataset_id = row[\"dataset_id\"]\n            citation_type = row[\"type\"]\n            if article_id not in true_labels:\n                true_labels[article_id] = {}\n            true_labels[article_id][dataset_id] = citation_type\n\n        X_dicts, y = [], []\n        print(\"Generating training samples (with negative sampling)...\")\n        for article_id, text in tqdm(documents.items()):\n            # Skip if this article_id is not in our labels file\n            if article_id not in true_labels:\n                continue\n\n            # Find all potential citations in the text\n            found_citations = self.extract_data_citations(text)\n            true_citations_for_article = true_labels.get(article_id, {})\n\n            for citation in found_citations:\n                features = self.create_features(text, citation)\n                X_dicts.append(features)\n\n                # Check if this found citation is a true label\n                if citation in true_citations_for_article:\n                    # It's a true positive, use the real label\n                    y.append(true_citations_for_article[citation])\n                else:\n                    # It's a false positive, label it as \"Not_A_Citation\"\n                    y.append(\"Not_A_Citation\")\n\n        if not X_dicts:\n            print(\"No training data found.\")\n            return\n\n        print(f\"Generated {len(X_dicts)} training samples.\")\n        print(pd.Series(y).value_counts())\n\n        X = self.vectorizer.fit_transform(X_dicts)\n        y_enc = self.label_encoder.fit_transform(y)\n\n        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n        scores = []\n        for train_idx, val_idx in skf.split(X, y_enc):\n            self.classifier.fit(X[train_idx], y_enc[train_idx])\n            preds = self.classifier.predict(X[val_idx])\n            scores.append(f1_score(y_enc[val_idx], preds, average=\"weighted\"))\n\n        print(f\"Cross-val F1: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n        self.classifier.fit(X, y_enc)\n\n    def predict(self, base_path):\n        print(\"Generating predictions...\")\n        documents = self.load_documents(base_path, \"test\")\n        predictions = []\n        row_id = 0\n\n        for article_id, text in tqdm(documents.items()):\n            citations = self.extract_data_citations(text)\n            for citation in citations:\n                features = self.create_features(text, citation)\n                X = self.vectorizer.transform([features])\n                pred_encoded = self.classifier.predict(X)[0]\n                citation_type = self.label_encoder.inverse_transform(\n                    [pred_encoded]\n                )[0]\n\n                \n                # Only include the prediction if it's NOT a \"Not_A_Citation\"\n                if citation_type != \"Not_A_Citation\":\n                    predictions.append(\n                        {\n                            \"row_id\": row_id,\n                            \"article_id\": article_id,\n                            \"dataset_id\": citation,\n                            \"type\": citation_type,\n                        }\n                    )\n                    row_id += 1\n\n        return pd.DataFrame(predictions)\n\n    def save_model(self, path):\n        joblib.dump(\n            {\n                \"classifier\": self.classifier,\n                \"label_encoder\": self.label_encoder,\n                \"vectorizer\": self.vectorizer,\n            },\n            path,\n        )\n\n    def load_model(self, path):\n        data = joblib.load(path)\n        self.classifier = data[\"classifier\"]\n        self.label_encoder = data[\"label_encoder\"]\n        self.vectorizer = data[\"vectorizer\"]\n\n\nif __name__ == \"__main__\":\n    base_path = \"/kaggle/input/make-data-count-finding-data-references\"\n    detector = DataCitationDetector()\n    detector.train_model(base_path)\n    df = detector.predict(base_path)\n    df.to_csv(\"submission.csv\", index=False)\n    print(f\"Saved {len(df)} predictions to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:09:52.108631Z","iopub.execute_input":"2025-06-15T09:09:52.109419Z","iopub.status.idle":"2025-06-15T09:13:48.996782Z","shell.execute_reply.started":"2025-06-15T09:09:52.109392Z","shell.execute_reply":"2025-06-15T09:13:48.996087Z"}},"outputs":[{"name":"stdout","text":"Training model...\nGenerating training samples (with negative sampling)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 524/524 [03:22<00:00,  2.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generated 2330 training samples.\nNot_A_Citation    2187\nPrimary             79\nSecondary           64\nName: count, dtype: int64\nCross-val F1: 0.9538 ± 0.0072\nGenerating predictions...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30/30 [00:14<00:00,  2.14it/s]","output_type":"stream"},{"name":"stdout","text":"Saved 5 predictions to submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Result: \n\nScore: 0.095\n\nRank: 56 (2025-06-15-18:55, JST)","metadata":{}}]}