{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"sourceType":"competition"},{"sourceId":248118764,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Original Notebook: \n\nhttps://www.kaggle.com/code/verracodeguacas/xml-inside-recall-boosted","metadata":{}},{"cell_type":"markdown","source":"# üìÑüßæ XML Inside, Recall Boosted! üöÄ\n\nThis notebook builds a complete extraction pipeline for dataset citations in scientific articles. It includes XML files so that we don't leave any information on the table. It is designed to be run **cell-by-cell**, In my opinion, making it much easier to understand, modify, and debug compared to pipeline-style scripts that wrap everything into `.py` files.\n\nThere are two new main contributions here:\n\n- **XML support**: Some articles in this dataset include useful metadata only in the XML files ‚Äî not the PDF. Including the XML gives a small but measurable improvement in score. If you‚Äôre aiming to squeeze out every last bit of performance, you‚Äôll need to include it. \n  \n- **Transparent execution**: All code runs directly in the notebook, with intermediate outputs shown in place. This structure helps with experimentation and troubleshooting, especially during development or error analysis.\n\nThroughout the notebook, you‚Äôll also see **diagnostic printouts** that helped surface edge cases and bugs during development; especially around ID cleaning and filtering. This part is still subject to improvement and tweaking which I haven't done thoroughly.\n\n> Inspiration for this approach came from [this excellent notebook](https://www.kaggle.com/code/mccocoful/notebook2d0b45c244). I‚Äôve adopted most of its best practices while adapting flexibility and XML handling.\n","metadata":{}},{"cell_type":"code","source":"# Install packages\n!uv pip install -q --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' pymupdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T03:05:30.413059Z","iopub.execute_input":"2025-07-21T03:05:30.413278Z","iopub.status.idle":"2025-07-21T03:05:31.626188Z","shell.execute_reply.started":"2025-07-21T03:05:30.413258Z","shell.execute_reply":"2025-07-21T03:05:31.624915Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## üîß Environment Setup and Constants\n\nWe begin by installing the required dependencies and importing all key libraries. The notebook uses `pymupdf` to parse PDFs, `lxml` to handle XML, and prefers `polars` over `pandas` for performance ‚Äî although we'll switch back to pandas in a few places where polars gave me trouble.\n\nWe also define a few helper functions for things like scoring, formatting DOIs, and resolving paths depending on whether we‚Äôre running locally or in the Kaggle environment.\n\nVerbose mode for Polars is turned on at this stage as well. This helps surface type coercions or lazy evaluation issues when chaining operations.\n","metadata":{}},{"cell_type":"code","source":"# Imports and Constants\nimport os, re, pathlib\nimport polars as pl\nfrom lxml import etree\nimport pymupdf\nfrom typing import Tuple\n\nDOI_URL = 'https://doi.org/'\n\n# Polars verbosity for debugging\npl.Config.set_verbose(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T03:05:31.628740Z","iopub.execute_input":"2025-07-21T03:05:31.628997Z","iopub.status.idle":"2025-07-21T03:05:33.220687Z","shell.execute_reply.started":"2025-07-21T03:05:31.628972Z","shell.execute_reply":"2025-07-21T03:05:33.219497Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"polars.config.Config"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Utilities and Helpers\n\ndef is_submission():\n    return bool(os.getenv('KAGGLE_IS_COMPETITION_RERUN'))\n\ndef is_kaggle_env():\n    return (len([k for k in os.environ.keys() if 'KAGGLE' in k]) > 0) or is_submission()\n\ndef get_prefix_path(prefix: str) -> pathlib.Path:\n    # Use correct directory based on environment\n    return pathlib.Path(f'/kaggle/{prefix}' if is_kaggle_env() else f'.{prefix}').expanduser().resolve()\n\ndef is_doi(name: str) -> pl.Expr:\n    return pl.col(name).str.starts_with(DOI_URL)\n\ndef doi_link_to_id(name: str) -> pl.Expr:\n    return pl.when(is_doi(name)).then(pl.col(name).str.split(DOI_URL).list.last()).otherwise(name).alias(name)\n\ndef doi_id_to_link(name: str, substring: str, url: str = DOI_URL) -> pl.Expr:\n    return pl.when(pl.col(name).str.starts_with(substring)).then(url + pl.col(name).str.to_lowercase()).otherwise(name).alias(name)\n\ndef score(preds: pl.DataFrame, gt: pl.DataFrame, on: list = ['article_id', 'dataset_id'], verbose: bool = True) -> Tuple[float, float, float]:\n    if 'id' in preds.columns and 'dataset_id' not in preds.columns:\n        preds = preds.rename({'id': 'dataset_id'})\n    hits = gt.join(preds, on=on)\n    tp = hits.height\n    fp = preds.height - tp\n    fn = gt.height - tp\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n\n    if verbose:\n        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n        print(f\"True Positives: {tp}, False Positives: {fp}, False Negatives: {fn}\")\n\n    return precision, recall, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T03:05:33.222654Z","iopub.execute_input":"2025-07-21T03:05:33.223255Z","iopub.status.idle":"2025-07-21T03:05:33.238235Z","shell.execute_reply.started":"2025-07-21T03:05:33.223210Z","shell.execute_reply":"2025-07-21T03:05:33.237094Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## üìÑ XML and PDF Parsing Functions\n\nThis section defines the core logic to convert PDFs and XMLs into plain text.\n\n- `pdf2text` reads each PDF using `pymupdf`, extracting text page by page.\n- `xml2text` uses `lxml` and applies different parsing logic depending on the XML schema. It handles common formats like TEI, JATS, Wiley, and BioC, and falls back to a generic method for unknown structures. I found that these formats existed thanks to this discussion: https://www.kaggle.com/competitions/make-data-count-finding-data-references/discussion/584638\n- A small normalization trick is applied to remove line breaks after certain DOI prefixes, which sometimes show up with awkward formatting (e.g. `10.\\n5061`).\n\nThe goal here is to get everything into a consistent, readable text format so we can scan for dataset IDs later. If both XML and PDF exist for an article, we append the XML content after the PDF ‚Äî no deduplication, just a simple merge.\n","metadata":{}},{"cell_type":"code","source":"# XML & PDF Parsing\n\ndef xml_kind(path: pathlib.Path) -> str:\n    head = path.open('rb').read(2048).decode('utf8', 'ignore')\n    if 'www.tei-c.org/ns' in head:\n        return 'tei'\n    if re.search(r'(NLM|TaxonX)//DTD', head):\n        return 'jats'\n    if 'www.wiley.com/namespaces' in head:\n        return 'wiley'\n    if 'BioC.dtd' in head:\n        return 'bioc'\n    return 'unknown'\n\ndef xml2text(path: pathlib.Path) -> str:\n    kind = xml_kind(path)\n    root = etree.parse(str(path)).getroot()\n    if kind in ('tei', 'bioc', 'unknown'):\n        txt = ' '.join(root.itertext())\n    elif kind == 'jats':\n        elems = root.xpath('//body//sec|//ref-list')\n        txt = ' '.join(' '.join(e.itertext()) for e in elems)\n    elif kind == 'wiley':\n        elems = root.xpath('//*[local-name()=\"body\"]|//*[local-name()=\"refList\"]')\n        txt = ' '.join(' '.join(e.itertext()) for e in elems)\n    else:\n        txt = ' '.join(root.itertext())\n    txt = re.sub(r'10\\.\\d{4,9}/\\s+', '10.', txt)\n    return txt\n\ndef pdf2text(path: pathlib.Path, out_dir: pathlib.Path) -> None:\n    doc = pymupdf.open(str(path))\n    out = out_dir / f\"{path.stem}.txt\"\n    with open(out, \"wb\") as f:\n        for page in doc:\n            f.write(page.get_text().encode(\"utf8\"))\n            f.write(b\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T03:05:33.239770Z","iopub.execute_input":"2025-07-21T03:05:33.240085Z","iopub.status.idle":"2025-07-21T03:05:33.276245Z","shell.execute_reply.started":"2025-07-21T03:05:33.240059Z","shell.execute_reply":"2025-07-21T03:05:33.275060Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## üõ†Ô∏è Bulk Parsing: PDFs and XMLs to Plain Text\n\nNow we bring the parsers together to process the full test set.\n\nThe function `parse_all_pdfs_xmls` scans the test directory, finds all `.pdf` and `.xml` files, and converts each one to a `.txt` file in a target `parsed` directory. \n\n- PDFs are processed first.\n- XMLs (if present) are appended to the same `.txt` file using binary append mode (`\"ab\"`), so the result includes both sources back-to-back. You may want to improve this by saving pdf's and xmls into separate folders and analyzing them separately, the sky is your limit!\n\nAny parsing errors are caught and logged with the article ID. This helps avoid crashing the entire run if a single file is corrupted.\n\nAt the end of this step, every article will have a `.txt` file with its text content ‚Äî pulled from PDF, XML, or both. (note that I didn't find any article ONLY in PDF format in the train/test set)\n","metadata":{}},{"cell_type":"code","source":"# Parse All PDFs & XMLs to TXT\nfrom tqdm.auto import tqdm\n\ndef parse_all_pdfs_xmls(pdf_dir, xml_dir, parsed_dir):\n    pdf_files = list(pdf_dir.glob('*.pdf'))\n    if not pdf_files and not xml_dir.exists():\n        raise ValueError(\"No PDF or XML files found.\")\n\n    parsed_dir.mkdir(parents=True, exist_ok=True)\n\n    # PDF ‚Üí TXT\n    for pdf in tqdm(pdf_files, desc=\"PDF‚ÜíTXT\"):\n        try:\n            pdf2text(pdf, parsed_dir)\n        except Exception as e:\n            print(f\"PDF error {pdf.stem}: {e}\")\n\n    # XML ‚Üí TXT (append mode)\n    if xml_dir.exists():\n        for xml in tqdm(xml_dir.glob('*.xml'), desc=\"XML‚ÜíTXT\"):\n            try:\n                txt = xml2text(xml).encode(\"utf8\")\n                out = parsed_dir / f\"{xml.stem}.txt\"\n                with open(out, \"ab\") as f:  # 'ab' = append binary\n                    f.write(txt)\n                    f.write(b\"\\n\")\n            except Exception as e:\n                print(f\"XML error {xml.stem}: {e}\")\n    print(\"Done parsing to text.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T03:05:33.277635Z","iopub.execute_input":"2025-07-21T03:05:33.277990Z","iopub.status.idle":"2025-07-21T03:05:33.515224Z","shell.execute_reply.started":"2025-07-21T03:05:33.277961Z","shell.execute_reply":"2025-07-21T03:05:33.514189Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## üîç Regex and Reference-Aware Text Preparation\n\nBefore we try to extract dataset IDs, we need to load all the parsed text files into a single DataFrame. This happens in the `get_text_df` function.\n\nHere‚Äôs what it does:\n\n- Reads all `.txt` files from the parsed directory.\n- Applies some Unicode normalization and removes non-ASCII characters.\n- Collapses multiple newlines and cleans up formatting.\n- Splits each article into two parts: the **body** and the **references**, based on where words like \"references\" or \"acknowledgments\" are detected (in reverse).\n  \nThe idea is to isolate references at the end of the paper, since that‚Äôs often where dataset citations appear. We store the result as a Polars DataFrame with three columns: `text`, `refs`, and `body`.\n\nThis layout will be helpful later when we want to compare where an ID appears ‚Äî in the body or in the references section.\n","metadata":{}},{"cell_type":"code","source":"# Extraction Helpers\n# This cell defines a regex for extracting dataset IDs from text,\n# and a helper function to read in all parsed .txt files as a DataFrame.\n\nimport matplotlib.pyplot as plt\nimport polars as pl\nfrom pathlib import Path\n\nREGEX_IDS = (\n    r\"(?i)\\b(?:\"\n    r\"CHEMBL\\d+|\"\n    r\"E-GEOD-\\d+|E-PROT-\\d+|EMPIAR-\\d+|\"\n    r\"ENSBTAG\\d+|ENSOARG\\d+|\"\n    r\"EPI_ISL_\\d{5,}|EPI\\d{6,7}|\"\n    r\"HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|KX\\d{6}|K0\\d{4}|\"\n    r\"PRJNA\\d+|PXD\\d+|SAMN\\d+|\"\n    r\"dryad\\.[^\\s\\\"<>]+|pasta\\/[^\\s\\\"<>]+\"\n    r\")\"\n)\n\n\ndef get_text_df(parsed_dir: Path) -> pl.DataFrame:\n    paths = list(parsed_dir.rglob('*.txt'))\n    records = [{'article_id': p.stem, 'text': p.read_text(encoding='utf8')} for p in paths]\n    return (\n        pl.DataFrame(records)\n        .with_columns(\n            pl.col(\"text\")\n              .str.normalize(\"NFKC\")\n              .str.replace_all(r\"[^\\p{Ascii}]\", \"\")\n        )\n        .with_columns(\n            pl.col(\"text\")\n              .str.split(r'\\n{2,}')\n              .list.eval(pl.col(\"\").str.replace_all('\\n', ' '))\n              .list.join('\\n')\n              .alias('text')\n        )\n        .with_columns([\n            pl.col(\"text\")\n              .str.slice(pl.col(\"text\").str.len_chars() // 4)\n              .str.reverse()\n              .alias('rtext'),\n            pl.col(\"text\")\n              .str.slice(0, pl.col(\"text\").str.len_chars() // 4)\n              .alias('ltext'),\n        ])\n        .with_columns(\n            pl.col(\"rtext\")\n              .str.find(r'(?i)\\b(secnerefer|erutaretil detic|stnemegdelwonkca)\\b')\n              .alias('ref_idx')\n        )\n        .with_columns(\n            pl.when(pl.col(\"ref_idx\").is_null()).then(0).otherwise(pl.col(\"ref_idx\")).alias(\"ref_idx\")\n        )\n        .with_columns([\n            pl.col(\"rtext\")\n              .str.slice(0, pl.col(\"ref_idx\"))\n              .str.reverse()\n              .alias(\"refs\"),\n            (pl.col(\"ltext\") + pl.col(\"rtext\").str.slice(pl.col(\"ref_idx\")).str.reverse()).alias(\"body\")\n        ])\n        .drop(\"rtext\", \"ltext\")\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T03:05:33.516648Z","iopub.execute_input":"2025-07-21T03:05:33.517037Z","iopub.status.idle":"2025-07-21T03:05:33.529084Z","shell.execute_reply.started":"2025-07-21T03:05:33.516970Z","shell.execute_reply":"2025-07-21T03:05:33.527989Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## üß¨ Candidate Dataset ID Extraction\n\nThis is the main engine of the notebook ‚Äî the `extract_candidates` function runs the full ID extraction pipeline.\n\nHere‚Äôs the step-by-step breakdown:\n\n- **[A] Extract candidate IDs** using a long regex that targets common dataset patterns (e.g., Dryad, PASTA, PRJNA, SAMN, etc.). This step is done with Polars, then converted to Pandas for flexibility.\n  \n- **[B] Explode**: Articles often contain multiple matches, so we explode the list of matches into separate rows ‚Äî one per candidate ID.\n\n- **[C] Clean**: IDs can be messy (extra punctuation, spacing). We strip out whitespace, punctuation at the end, and standardize casing.\n\n- **[D] Normalize DOIs**: Dryad and PASTA references are turned into full DOI links using their respective prefixes.\n\n- **[E] Choose the best version**: Prefer a canonical DOI if one is available; otherwise, fall back to the cleaned version.\n\n- **[F] Filter**: This step does the heavy lifting:\n  - Drop nulls and self-citations.\n  - Remove false positives like `figshare`.\n  - Enforce minimum DOI suffix length.\n  - Drop known stub DOIs that appear in boilerplate text.\n  - Ensure parentheses and brackets are balanced ‚Äî a common issue in malformed references.\n\n‚ö†Ô∏è Note: At this point, I ran into some compatibility issues in Polars, especially with filtering conditions that required context across multiple columns. For that reason, I moved this logic to Pandas ‚Äî it was more stable and easier to debug. I'll keep trying to make this notebook pandas free in the future, but for now I decided to publish it like that.\n\n- **[G] Extract context window**: For each surviving candidate, we grab a chunk of text around the ID. This will later be used by the classifier (or for manual inspection).\n\nThe final DataFrame contains three columns: `article_id`, `dataset_id`, and `window`.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom collections import Counter\n\ndef extract_candidates(args):\n    parsed_in = get_prefix_path(\"working\") / args['i']\n    print(f\"üîµ Step‚ÄØ2: Begin ID Extraction Pipeline\")\n    print(f\"   ‚Üí Will process parsed text files from: {parsed_in}\")\n    \n    # Start from polars then convert to pandas for further steps\n    text_df = get_text_df(parsed_in)\n    print(f\"üü¢ Step‚ÄØ1: Loaded text DataFrame\")\n    print(f\"   ‚Üí Rows: {text_df.height}, Columns: {list(text_df.columns)}\")\n    print(text_df.with_columns(pl.col(\"text\").str.slice(0, 100).alias(\"text_snippet\")).head(2).to_pandas())\n\n    # Step A: Extract candidate IDs (regex)\n    df = text_df.with_columns(pl.col(\"text\").str.extract_all(REGEX_IDS).alias(\"id\")).to_pandas()\n    print(f\"üü¶ [A] Extract candidate IDs\")\n    print(df[[\"article_id\", \"id\"]].head(2))\n\n    # Step B: Explode for one row per candidate\n    df = df.explode(\"id\").rename(columns={\"id\": \"match_id\"})\n    print(f\"üü¶ [B] Exploded IDs\")\n    print(df[[\"article_id\", \"match_id\"]].head(2))\n\n    # Step C: Clean IDs\n    df[\"id\"] = df[\"match_id\"]\n    df[\"id_nospace\"] = df[\"id\"].str.replace(r\"\\s+\", \"\", regex=True)\n    df[\"id_cleaned\"] = df[\"id_nospace\"].str.replace(r\"[-.,;:!?/)\\]\\(\\[]+$\", \"\", regex=True)\n    print(f\"üü¶ [C] Cleaned IDs\")\n    print(df[[\"article_id\", \"id\", \"id_cleaned\"]].head(2))\n\n    # Step D: Expand DOIs\n    def norm_dryad(x):\n        return f\"https://doi.org/10.5061/{x.lower()}\" if isinstance(x, str) and x.startswith(\"dryad.\") else None\n    def norm_pasta(x):\n        return f\"https://doi.org/10.6073/{x.lower()}\" if isinstance(x, str) and x.startswith(\"pasta/\") else None\n\n    df[\"id_final_dryad\"] = df[\"id_cleaned\"].map(norm_dryad)\n    df[\"id_final_pasta\"] = df[\"id_cleaned\"].map(norm_pasta)\n    print(f\"üü¶ [D] Normalized DOIs (dryad/pasta)\")\n    print(df[[\"article_id\", \"id_final_dryad\", \"id_final_pasta\"]].head(2))\n\n    # Step E: Prioritize full DOI URL, fallback to cleaned\n    df[\"id_use\"] = df[\"id_final_dryad\"].combine_first(df[\"id_final_pasta\"]).combine_first(df[\"id_cleaned\"])\n    print(f\"üü¶ [E] Chose ID to use\")\n    print(df[[\"article_id\", \"id_use\"]].head(2))\n\n    # Step F: Filter false positives (Enhanced)\n    # -- Drop nulls\n    df = df[df[\"id_use\"].notnull()]\n    # -- Remove IDs that include the article's own ID\n    df = df[~df.apply(lambda row: str(row[\"article_id\"]).replace(\"_\", \"/\").lower() in str(row[\"id_use\"]).lower(), axis=1)]\n    # -- Remove 'figshare'\n    df = df[~df[\"id_use\"].str.contains(\"figshare\", na=False)]\n    # -- Remove DOIs with short suffixes\n    def valid_doi(x):\n        if isinstance(x, str) and x.startswith(DOI_URL):\n            return len(x.rsplit(\"/\", 1)[-1]) >= 4\n        return True\n    df = df[df[\"id_use\"].apply(valid_doi)]\n    # -- Remove stub DOIs\n    STUBS = [\"https://doi.org/10.5061/dryad\", \"https://doi.org/10.6073/pasta\", \"https://doi.org/10.5281/zenodo\"]\n    df = df[~df[\"id_use\"].isin(STUBS)]\n    # -- Paren/bracket matching\n    df = df[df[\"id_use\"].str.count(r\"\\(\") == df[\"id_use\"].str.count(r\"\\)\")]\n    df = df[df[\"id_use\"].str.count(r\"\\[\") == df[\"id_use\"].str.count(r\"\\]\")]\n    print(f\"üü¶ [F] Filtered false positives (showing a few):\")\n    print(df[[\"article_id\", \"id_use\"]].head(5))\n\n    # Step G: Extract window context and rename\n    def get_window(row):\n        idx = row[\"text\"].find(row[\"id_use\"])\n        if idx == -1:\n            return \"\"\n        start = max(idx - args['ws'] - len(str(row[\"id_use\"])), 0)\n        end = idx + args['ws'] + len(str(row[\"id_use\"]))\n        return row[\"text\"][start:end]\n    df[\"window\"] = df.apply(get_window, axis=1)\n    df = df[[\"article_id\", \"id_use\", \"window\"]].drop_duplicates().rename(columns={\"id_use\": \"dataset_id\"})\n    print(f\"\\n‚úÖ Completed extraction: {len(df)} unique (article_id, dataset_id) pairs\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T03:05:33.531417Z","iopub.execute_input":"2025-07-21T03:05:33.532400Z","iopub.status.idle":"2025-07-21T03:05:34.011856Z","shell.execute_reply.started":"2025-07-21T03:05:33.532360Z","shell.execute_reply":"2025-07-21T03:05:34.010563Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## üöÄ Main Pipeline: Run It All\n\nThis final function ties everything together.\n\n- **Step 1**: We parse all PDFs and XMLs using `parse_all_pdfs_xmls`, converting them into `.txt` files inside the `parsed` folder.\n- **Step 2**: We call `extract_candidates`, which loads those text files and runs the full ID extraction and filtering pipeline.\n- The results are saved in both `.parquet` and `.csv` formats ‚Äî ready for submission.\n\nWe also tag each `dataset_id` as either `\"Primary\"` or `\"Secondary\"`:\n- A `\"Primary\"` ID is a proper DOI or something like a `SAMN` identifier.\n- Everything else is treated as `\"Secondary\"` ‚Äî probably useful, but not canonical.\n\nFinally, the code builds a Kaggle submission file with the expected format: `row_id`, `article_id`, `dataset_id`, and `type`.\n\nEverything prints as it runs ‚Äî no surprises, no hidden state.\n\n‚úÖ When you run this cell, your entire pipeline runs end-to-end, and the final submission is saved to disk.\n","metadata":{}},{"cell_type":"code","source":"# Cell 8: Main Pipeline, concise output\ndef main_pipeline():\n    args = {'i': 'parsed', 'o': 'extracted_ids.parquet', 'gt': 'make-data-count-finding-data-references/train_labels.csv', 'ws': 100}\n\n    print(\"üåü STEP 1: Parse all PDFs and XMLs to text files\")\n    pdf_dir = pathlib.Path('/kaggle/input/make-data-count-finding-data-references/test/PDF')\n    xml_dir = pathlib.Path('/kaggle/input/make-data-count-finding-data-references/test/XML')\n    parsed_dir = get_prefix_path('working') / args['i']\n    print(parsed_dir)\n    parse_all_pdfs_xmls(pdf_dir, xml_dir, parsed_dir)\n\n    print(\"\\nüåü STEP 2: Extract candidate dataset IDs from text\")\n    df = extract_candidates(args)\n    out_parq = get_prefix_path(\"working\") / args['o']\n    df.to_parquet(out_parq)\n    print(f\"‚úî Saved extracted IDs to: {out_parq} ‚Äî {len(df)} rows\")\n\n    # Build submission DataFrame with 'type'\n    def assign_type(x):\n        if isinstance(x, str) and (x.startswith(DOI_URL) or x.startswith(\"SAMN\")):\n            return \"Primary\"\n        else:\n            return \"Secondary\"\n    sub = df.copy()\n    sub[\"type\"] = sub[\"dataset_id\"].apply(assign_type)\n    sub = sub.drop_duplicates(subset=[\"article_id\", \"dataset_id\"]).reset_index(drop=True)\n    sub[\"row_id\"] = range(len(sub))\n    sub = sub[[\"row_id\", \"article_id\", \"dataset_id\", \"type\"]]\n    print(\"\\n[main_pipeline] Submission DataFrame (first rows):\")\n    print(sub.head())\n\n    sub.to_csv(get_prefix_path(\"working\") / \"submission.csv\", index=False)\n    print(f\"‚úî Submission saved ‚Äî {len(sub)} rows\")\n\n    # Optionally: add scoring and validation if running on train split\n\n    print(\"\\n‚úÖ Pipeline finished!\")\n\nmain_pipeline()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T03:05:34.013031Z","iopub.execute_input":"2025-07-21T03:05:34.013635Z","iopub.status.idle":"2025-07-21T03:05:40.547874Z","shell.execute_reply.started":"2025-07-21T03:05:34.013590Z","shell.execute_reply":"2025-07-21T03:05:40.546821Z"}},"outputs":[{"name":"stdout","text":"üåü STEP 1: Parse all PDFs and XMLs to text files\n/kaggle/working/parsed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"PDF‚ÜíTXT:   0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3688b47f092e4ea0bfabb450d34dead1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"XML‚ÜíTXT: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd2ce262a4540d58c3d0e161c06b892"}},"metadata":{}},{"name":"stdout","text":"Done parsing to text.\n\nüåü STEP 2: Extract candidate dataset IDs from text\nüîµ Step‚ÄØ2: Begin ID Extraction Pipeline\n   ‚Üí Will process parsed text files from: /kaggle/working/parsed\nüü¢ Step‚ÄØ1: Loaded text DataFrame\n   ‚Üí Rows: 30, Columns: ['article_id', 'text', 'ref_idx', 'refs', 'body']\n          article_id                                               text  \\\n0  10.1002_ece3.9627  Ecology and Evolution. 2022;12:e9627.      | 1...   \n1   10.1002_mp.14424  PleThora: Pleural effusion and thoracic cavity...   \n\n   ref_idx                                               refs  \\\n0    18389       Amir ,  Z.   ,    Moore ,  J. H.   ,    N...   \n1    14620   1     Kumar   V   ,    Gu   Y   ,    Basu   S...   \n\n                                                body  \\\n0  Ecology and Evolution. 2022;12:e9627.      | 1...   \n1  PleThora: Pleural effusion and thoracic cavity...   \n\n                                        text_snippet  \n0  Ecology and Evolution. 2022;12:e9627.      | 1...  \n1  PleThora: Pleural effusion and thoracic cavity...  \nüü¶ [A] Extract candidate IDs\n          article_id                                                 id\n0  10.1002_ece3.9627  [dryad.b8gtht7h3., dryad.b8gtht7h3, dryad.b8gt...\n1   10.1002_mp.14424                                                 []\nüü¶ [B] Exploded IDs\n          article_id          match_id\n0  10.1002_ece3.9627  dryad.b8gtht7h3.\n0  10.1002_ece3.9627   dryad.b8gtht7h3\nüü¶ [C] Cleaned IDs\n          article_id                id       id_cleaned\n0  10.1002_ece3.9627  dryad.b8gtht7h3.  dryad.b8gtht7h3\n0  10.1002_ece3.9627   dryad.b8gtht7h3  dryad.b8gtht7h3\nüü¶ [D] Normalized DOIs (dryad/pasta)\n          article_id                           id_final_dryad id_final_pasta\n0  10.1002_ece3.9627  https://doi.org/10.5061/dryad.b8gtht7h3           None\n0  10.1002_ece3.9627  https://doi.org/10.5061/dryad.b8gtht7h3           None\nüü¶ [E] Chose ID to use\n          article_id                                   id_use\n0  10.1002_ece3.9627  https://doi.org/10.5061/dryad.b8gtht7h3\n0  10.1002_ece3.9627  https://doi.org/10.5061/dryad.b8gtht7h3\nüü¶ [F] Filtered false positives (showing a few):\n          article_id                                   id_use\n0  10.1002_ece3.9627  https://doi.org/10.5061/dryad.b8gtht7h3\n0  10.1002_ece3.9627  https://doi.org/10.5061/dryad.b8gtht7h3\n0  10.1002_ece3.9627  https://doi.org/10.5061/dryad.b8gtht7h3\n3  10.1002_ecs2.4619   PASTA/D835832D7FD00D9E4466E44EEA87FAB3\n3  10.1002_ecs2.4619   PASTA/D835832D7FD00D9E4466E44EEA87FAB3\n\n‚úÖ Completed extraction: 9 unique (article_id, dataset_id) pairs\n‚úî Saved extracted IDs to: /kaggle/working/extracted_ids.parquet ‚Äî 9 rows\n\n[main_pipeline] Submission DataFrame (first rows):\n   row_id              article_id                               dataset_id  \\\n0       0       10.1002_ece3.9627  https://doi.org/10.5061/dryad.b8gtht7h3   \n1       1       10.1002_ecs2.4619   PASTA/D835832D7FD00D9E4466E44EEA87FAB3   \n2       2       10.1002_ece3.6303  https://doi.org/10.5061/dryad.37pvmcvgb   \n3       3  10.1002_chem.202001668                                   K03946   \n4       4  10.1002_chem.202001668                                   K01438   \n\n        type  \n0    Primary  \n1  Secondary  \n2    Primary  \n3  Secondary  \n4  Secondary  \n‚úî Submission saved ‚Äî 9 rows\n\n‚úÖ Pipeline finished!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def show_submission(sub_csv='/kaggle/working/submission.csv'):\n    df = pd.read_csv(sub_csv)\n    df = df.reset_index(drop=True)\n    df['row_id'] = df.index\n    print(df[['row_id', 'article_id', 'dataset_id', 'type']].to_string(index=False))\n\nshow_submission()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T03:05:40.548927Z","iopub.execute_input":"2025-07-21T03:05:40.549213Z","iopub.status.idle":"2025-07-21T03:05:40.564299Z","shell.execute_reply.started":"2025-07-21T03:05:40.549190Z","shell.execute_reply":"2025-07-21T03:05:40.563355Z"}},"outputs":[{"name":"stdout","text":" row_id             article_id                              dataset_id      type\n      0      10.1002_ece3.9627 https://doi.org/10.5061/dryad.b8gtht7h3   Primary\n      1      10.1002_ecs2.4619  PASTA/D835832D7FD00D9E4466E44EEA87FAB3 Secondary\n      2      10.1002_ece3.6303 https://doi.org/10.5061/dryad.37pvmcvgb   Primary\n      3 10.1002_chem.202001668                                  K03946 Secondary\n      4 10.1002_chem.202001668                                  K01438 Secondary\n      5      10.1002_ece3.4466   https://doi.org/10.5061/dryad.r6nq870   Primary\n      6      10.1002_ece3.5260   https://doi.org/10.5061/dryad.2f62927   Primary\n      7      10.1002_ece3.6144 https://doi.org/10.5061/dryad.zw3r22854   Primary\n      8      10.1002_ecs2.1280     https://doi.org/10.5061/dryad.p3fg9   Primary\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"! rm -rf parsed\n! rm -rf src\n! rm -rf extracted_ids.parquet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T03:05:40.565177Z","iopub.execute_input":"2025-07-21T03:05:40.565499Z","iopub.status.idle":"2025-07-21T03:05:40.959006Z","shell.execute_reply.started":"2025-07-21T03:05:40.565469Z","shell.execute_reply":"2025-07-21T03:05:40.957685Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Result:\n\nScore: 0.521\n\nRank: 74 (2025-07-21-12:31, JST)\n\nRuntime: 2min (kaggle editor), 15min (Scoring)\n\nYour Best Entry!\nYour most recent submission scored 0.521, which is an improvement of your previous score of 0.520. Great job!\n\nMoving up to rank 74. rising like my electricity bill. #kaggle - https://kaggle.com/competitions/make-data-count-finding-data-references ","metadata":{}},{"cell_type":"markdown","source":"Make Data Count, Maggie Demkin, and Walter Reade. Make Data Count - Finding Data References. https://kaggle.com/competitions/make-data-count-finding-data-references, 2025. Kaggle.","metadata":{}}]}